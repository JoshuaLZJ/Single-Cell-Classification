{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc18866d-16a5-4e5d-9386-a1866dab275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook trains a GCN autoencoder network first before combining the encoder network with another FF network \n",
    "#to predict single cell identities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53403aab-24b1-418e-894b-9aaefda8a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch_geometric.utils import get_laplacian\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "455cb024-d9f3-4fe8-a78f-7d569d4828d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading adjacency matrix and filtered gene expression\n",
    "\n",
    "adj_mat = np.load('data/adj_mat_on_normalized2.npy')\n",
    "pbmc = np.load('data/pbmc_normalized_filtered2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "307f9d52-3145-486e-b137-f593a1d8d80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = adj_mat.shape[0] # number of nodes in a graph\n",
    "D = np.sum(adj_mat, 0) # node degrees\n",
    "D_hat_pos = np.diag((D + 1e-5)**(0.5)) # normalized node degrees\n",
    "D_hat_neg = np.diag((D + 1e-5)**(-0.5)) # normalized node degrees\n",
    "L = np.identity(N) + D_hat_neg.dot(adj_mat).dot(D_hat_pos) # Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab785910-3ca8-4faa-8ec1-1bfddf15db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral convolution on graphs\n",
    "# X is an N×1 matrix of 1-dimensional node features\n",
    "# L is an N×N graph Laplacian computed above\n",
    "# W_spectral are N×F weights (filters) that we want to train\n",
    "from scipy.sparse.linalg import eigsh # assumes L to be symmetric\n",
    "\n",
    "Λ,V = eigsh(L,k=20,which='SM') # eigen-decomposition (i.e. find Λ,V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7defe08-8e4a-40ea-b08b-7ba59074be83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 960)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0bfa8e7-2600-47b2-82a9-028ba569bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoencoder model\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, in_features, out_features, batch_size, bias=False):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(self.batch_size, self.in_features, self.out_features), requires_grad=True)\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(self.batch_size, self.input_dim, self.out_features), requires_grad=True)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "#         stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "#         self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.xavier_uniform_(self.bias, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "#         input = torch.permute(input, (0, 2, 1))\n",
    "        support = torch.bmm(input, self.weight)\n",
    "        output = torch.matmul(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "    \n",
    "class autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, nhidden, p, adj, batch_size, dropout):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.nhidden = nhidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.poolsize = 8\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.adj = Parameter(torch.from_numpy(adj).contiguous().float(), requires_grad=False).to(self.device)\n",
    "        self.p = p\n",
    "        self.gcn = GraphConvolution(self.input_dim, 1, self.nhidden, self.batch_size, bias=False)\n",
    "        self.gcn2 = GraphConvolution(self.input_dim, self.nhidden, self.nhidden//2, self.batch_size, bias=False)\n",
    "        self.encoder_a1 = nn.Linear(int(((self.input_dim//self.p)*nhidden)), 128, bias=True)\n",
    "        self.encoder_a2 = nn.Linear(128, 32, bias=True)\n",
    "        \n",
    "        self.decoder1 = nn.Linear(32, 128, bias=True)\n",
    "        self.decoder2 = nn.Linear(128, self.input_dim, bias=True)\n",
    "        \n",
    "        #Make sure gene order in adj is same!\n",
    "        #how to implement the attention layer?\n",
    "        #what's the diff between Parameter and Variable\n",
    "        \n",
    "    def graph_max_pool(self, x, p):\n",
    "        if p > 1:\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x F x V\n",
    "            x = nn.MaxPool1d(p)(x)             # B x F x V/p\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x V/p x F\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #encoder\n",
    "        stream1 = torch.unsqueeze(x, -1)\n",
    "        stream1 = F.relu(self.gcn(stream1, self.adj))\n",
    "#         stream1 = F.relu(self.gcn2(stream1, self.adj))\n",
    "        stream1 = self.graph_max_pool(stream1, self.p)\n",
    "        stream1 = torch.flatten(stream1, start_dim = 1)\n",
    "        stream1 = F.relu(self.encoder_a1(stream1))\n",
    "        stream1 = F.relu(self.encoder_a2(stream1))\n",
    "        \n",
    "#         stream2 = F.relu(self.encoder_b1(x))\n",
    "#         stream2 = F.relu(self.encoder_b2(stream2))\n",
    "        \n",
    "#         combined = torch.cat((stream1, stream2), -1)\n",
    "        \n",
    "        #decoder\n",
    "        stream1 = F.relu(self.decoder1(stream1))\n",
    "        outputs = F.relu(self.decoder2(stream1))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b156b5f8-33b2-458f-8248-2b3329bcd7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoencoder using attention\n",
    "\n",
    "class GraphAttConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, in_features, out_features, batch_size, bias=False):\n",
    "        super(GraphAttConvolution, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.att = Parameter(torch.FloatTensor(self.input_dim, self.input_dim), requires_grad=True)\n",
    "        self.weight = Parameter(torch.FloatTensor(self.in_features, self.out_features), requires_grad=True)\n",
    "#         self.att = Parameter(torch.FloatTensor(self.input_dim, self.out_features), requires_grad=False)\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(self.input_dim, self.out_features), requires_grad=True)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        torch.nn.init.xavier_uniform_(self.att, gain=nn.init.calculate_gain('relu'))\n",
    "#         stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "#         self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.xavier_uniform_(self.bias, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "#         input = torch.permute(input, (0, 2, 1))\n",
    "        self.att = Parameter(torch.softmax(self.att, dim=1), requires_grad=True)\n",
    "        adj = torch.matmul(self.att, adj)\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.matmul(adj, support)\n",
    "#         att = torch.softmax(output, dim=1)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "    \n",
    "class autoencoder_att(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, nhidden, p, adj, batch_size, dropout):\n",
    "        super(autoencoder_att, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.nhidden = nhidden\n",
    "#         self.fc1 = nn.Linear(input_dim, nhidden, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.poolsize = 8\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.adj = Parameter(torch.from_numpy(adj).contiguous().float(), requires_grad=False).to(self.device)\n",
    "        self.p = p\n",
    "        self.gcn = GraphAttConvolution(self.input_dim, 1, self.nhidden, self.batch_size, bias=False)\n",
    "#         self.gcn2 = GraphAttConvolution(self.input_dim, self.nhidden, self.nhidden//2, self.batch_size, bias=False)\n",
    "        self.encoder_a1 = nn.Linear(int(((self.input_dim//self.p)*nhidden)), 128, bias=True)\n",
    "        self.encoder_a2 = nn.Linear(128, 32, bias=True)\n",
    "        \n",
    "        self.decoder1 = nn.Linear(32, 128, bias=True)\n",
    "        self.decoder2 = nn.Linear(128, self.input_dim, bias=True)\n",
    "        \n",
    "        \n",
    "        #Make sure gene order in adj is same!\n",
    "        #how to implement the attention layer?\n",
    "        #what's the diff between Parameter and Variable\n",
    "        \n",
    "    def graph_max_pool(self, x, p):\n",
    "        if p > 1:\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x F x V\n",
    "            x = nn.MaxPool1d(p)(x)             # B x F x V/p\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x V/p x F\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #encoder\n",
    "        stream1 = torch.unsqueeze(x, -1)\n",
    "        stream1 = F.relu(self.gcn(stream1, self.adj))\n",
    "#         stream1 = F.relu(self.gcn2(stream1, self.adj))\n",
    "        stream1 = self.graph_max_pool(stream1, self.p)\n",
    "        stream1 = torch.flatten(stream1, start_dim = 1)\n",
    "        stream1 = F.relu(self.encoder_a1(stream1))\n",
    "        stream1 = F.relu(self.encoder_a2(stream1))\n",
    "        \n",
    "        #decoder\n",
    "        stream1 = F.relu(self.decoder1(stream1))\n",
    "        outputs = F.relu(self.decoder2(stream1))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ebf020d-32fb-4038-a1fc-8862277e1901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoencoder using spectral theory model\n",
    "\n",
    "class SpectralConvolution(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, out_features, batch_size, bias=False):\n",
    "        super(SpectralConvolution, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.out_features = out_features\n",
    "        self.weight_spectral = Parameter(torch.FloatTensor(self.input_dim, self.out_features), requires_grad=True)\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(self.batch_size, self.input_dim, self.out_features), requires_grad=True)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight_spectral, gain=nn.init.calculate_gain('relu'))\n",
    "#         stdv = 1. / math.sqrt(self.weight_spectral.size(1))\n",
    "#         self.weight_spectral.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.xavier_uniform_(self.bias, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, input, V):\n",
    "        X_hat = torch.matmul(V.T, input) # 20×1 node features in the \"spectral\" domain\n",
    "        W_hat = torch.matmul(V.T, self.weight_spectral) # 20×F filters in the \"spectral\" domain\n",
    "        output = torch.matmul(V, X_hat * W_hat)  # N×F result of convolution\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.out_features) + ')'\n",
    "    \n",
    "class spectral_autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, nhidden, p, adj, batch_size, dropout):\n",
    "        super(spectral_autoencoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.nhidden = nhidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.poolsize = 8\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.adj = Parameter(torch.from_numpy(adj).contiguous().float(), requires_grad=False).to(self.device)\n",
    "        self.p = p\n",
    "        self.gcn = SpectralConvolution(self.input_dim, self.nhidden, self.batch_size, bias=False)\n",
    "        self.gcn2 = SpectralConvolution(self.input_dim, self.nhidden, self.batch_size, bias=False)\n",
    "        self.encoder_a1 = nn.Linear(int(((self.input_dim//self.p)*nhidden)), 128, bias=True)\n",
    "        self.encoder_a2 = nn.Linear(128, 32, bias=True)\n",
    "        \n",
    "        self.decoder1 = nn.Linear(32, 128, bias=True)\n",
    "        self.decoder2 = nn.Linear(128, self.input_dim, bias=True)\n",
    "        \n",
    "        #Make sure gene order in adj is same!\n",
    "        #how to implement the attention layer?\n",
    "        #what's the diff between Parameter and Variable\n",
    "        \n",
    "    def graph_max_pool(self, x, p):\n",
    "        if p > 1:\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x F x V\n",
    "            x = nn.MaxPool1d(p)(x)             # B x F x V/p\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x V/p x F\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #encoder\n",
    "        stream1 = torch.unsqueeze(x, -1)\n",
    "        stream1 = F.relu(self.gcn(stream1, self.adj))\n",
    "        stream1 = F.relu(self.gcn2(stream1, self.adj))\n",
    "        stream1 = self.graph_max_pool(stream1, self.p)\n",
    "        stream1 = torch.flatten(stream1, start_dim = 1)\n",
    "        stream1 = F.relu(self.encoder_a1(stream1))\n",
    "        stream1 = self.dropout(stream1)\n",
    "        stream1 = F.relu(self.encoder_a2(stream1))\n",
    "        \n",
    "        #decoder\n",
    "        stream1 = F.relu(self.decoder1(stream1))\n",
    "        stream1 = self.dropout(stream1)\n",
    "        outputs = F.relu(self.decoder2(stream1))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd2b6f7b-2f89-4afb-8900-d5b6cab1e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleAE(nn.Module):\n",
    "    def __init__(self,input_dim,device):\n",
    "        super(simpleAE, self).__init__()\n",
    "        self.fc_input=nn.Linear(input_dim,128)\n",
    "        self.fc1=nn.Linear(128,32)\n",
    "        self.fc2=nn.Linear(32,128)\n",
    "        self.fc3=nn.Linear(128,input_dim)\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "        self.device=device\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x.to(self.device)\n",
    "        x=F.relu(self.fc_input(x))\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        outputs=F.relu(self.fc3(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a46706ed-3676-4603-9122-c788c84ee8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#GCN AE\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = spectral_autoencoder(pbmc.shape[1], nhidden=32, p=8, adj=V, batch_size=batch_size, dropout=0.3)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a90489a1-0162-4a91-ae98-803175214fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj\n",
      "gcn.weight\n",
      "gcn2.weight\n",
      "encoder_a1.weight\n",
      "encoder_a1.bias\n",
      "encoder_a2.weight\n",
      "encoder_a2.bias\n",
      "decoder1.weight\n",
      "decoder1.bias\n",
      "decoder2.weight\n",
      "decoder2.bias\n"
     ]
    }
   ],
   "source": [
    "for k,v in model.state_dict().items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cc9f164-eb22-44a3-a67d-3582db6b3869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#SimpleAE\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = simpleAE(pbmc.shape[1], device=device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eae69f9-2b1b-499f-9dc9-5dd188baf6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(),\n",
    "#                              lr = 1e-2,\n",
    "#                              weight_decay = 1e-8)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                             lr = 1e-3)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50000, gamma=0.1)\n",
    "\n",
    "x_tensor = torch.tensor(pbmc[:len(pbmc)].astype(np.float32))\n",
    "\n",
    "dataset = TensorDataset(x_tensor, x_tensor)\n",
    "\n",
    "# batch_size = 32\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 30\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(x_tensor)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "190bf863-09cd-4df0-8a7a-b2957b72f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, scheduler=None):\n",
    "    size = math.floor(len(dataloader.dataset)*0.8)\n",
    "    model.train()\n",
    "    correct = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            correct /= 100*batch_size\n",
    "\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            correct = 0\n",
    "            \n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = math.floor(len(dataloader.dataset)*0.2)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a35766d8-3e19-45c3-ae57-3f8db930acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model(torch.tensor(pbmc[0:100].astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "66d6f3ca-24dd-4b99-93af-f6a8d444ce17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 99.5927, 165.9684,  83.0343, 124.8530,  74.8347, 116.0187,  33.0841,\n",
      "        157.9944, 124.0977, 141.3166], grad_fn=<SliceBackward0>)\n",
      "tensor([ 95.3137, 158.8562,  79.4281, 119.1422,  71.4853, 111.1994,  31.7712,\n",
      "        150.9134, 119.1422, 135.0278])\n"
     ]
    }
   ],
   "source": [
    "for X, y in validation_loader:\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    pred = model(X)\n",
    "    print(pred[10][0:10])\n",
    "    print(y[10][0:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8665769-4860-45cc-958b-4a693e66257f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 561.179932  [    0/52754]\n",
      "loss: 560.909912  [ 3200/52754]\n",
      "loss: 560.664856  [ 6400/52754]\n",
      "loss: 560.305847  [ 9600/52754]\n",
      "loss: 559.792664  [12800/52754]\n",
      "loss: 559.222839  [16000/52754]\n",
      "loss: 558.446228  [19200/52754]\n",
      "loss: 557.412170  [22400/52754]\n",
      "loss: 555.661682  [25600/52754]\n",
      "loss: 553.024353  [28800/52754]\n",
      "loss: 546.533813  [32000/52754]\n",
      "loss: 525.017700  [35200/52754]\n",
      "loss: 65.399437  [38400/52754]\n",
      "loss: 56.758110  [41600/52754]\n",
      "loss: 50.562012  [44800/52754]\n",
      "loss: 42.217869  [48000/52754]\n",
      "loss: 46.292110  [51200/52754]\n",
      "Avg loss: 26.590253 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 40.527897  [    0/52754]\n",
      "loss: 30.902571  [ 3200/52754]\n",
      "loss: 36.361240  [ 6400/52754]\n",
      "loss: 33.271950  [ 9600/52754]\n",
      "loss: 31.900114  [12800/52754]\n",
      "loss: 30.193987  [16000/52754]\n",
      "loss: 34.729111  [19200/52754]\n",
      "loss: 31.220364  [22400/52754]\n",
      "loss: 29.825640  [25600/52754]\n",
      "loss: 25.102697  [28800/52754]\n",
      "loss: 31.766535  [32000/52754]\n",
      "loss: 26.691778  [35200/52754]\n",
      "loss: 29.366869  [38400/52754]\n",
      "loss: 26.210039  [41600/52754]\n",
      "loss: 15.372524  [44800/52754]\n",
      "loss: 16.268293  [48000/52754]\n",
      "loss: 15.717040  [51200/52754]\n",
      "Avg loss: 11.393014 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 28.350075  [    0/52754]\n",
      "loss: 16.789221  [ 3200/52754]\n",
      "loss: 17.922344  [ 6400/52754]\n",
      "loss: 19.990589  [ 9600/52754]\n",
      "loss: 18.056360  [12800/52754]\n",
      "loss: 21.299829  [16000/52754]\n",
      "loss: 20.886938  [19200/52754]\n",
      "loss: 18.054211  [22400/52754]\n",
      "loss: 14.016666  [25600/52754]\n",
      "loss: 14.697317  [28800/52754]\n",
      "loss: 15.455173  [32000/52754]\n",
      "loss: 15.840625  [35200/52754]\n",
      "loss: 17.815681  [38400/52754]\n",
      "loss: 15.937110  [41600/52754]\n",
      "loss: 13.341762  [44800/52754]\n",
      "loss: 13.450787  [48000/52754]\n",
      "loss: 11.663500  [51200/52754]\n",
      "Avg loss: 4.880569 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 14.153421  [    0/52754]\n",
      "loss: 17.545479  [ 3200/52754]\n",
      "loss: 11.601922  [ 6400/52754]\n",
      "loss: 14.596666  [ 9600/52754]\n",
      "loss: 11.275919  [12800/52754]\n",
      "loss: 14.816851  [16000/52754]\n",
      "loss: 13.466289  [19200/52754]\n",
      "loss: 12.568534  [22400/52754]\n",
      "loss: 9.535644  [25600/52754]\n",
      "loss: 10.609372  [28800/52754]\n",
      "loss: 12.777216  [32000/52754]\n",
      "loss: 12.064536  [35200/52754]\n",
      "loss: 9.471876  [38400/52754]\n",
      "loss: 13.129136  [41600/52754]\n",
      "loss: 10.384715  [44800/52754]\n",
      "loss: 9.657392  [48000/52754]\n",
      "loss: 12.319810  [51200/52754]\n",
      "Avg loss: 3.867630 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 10.756317  [    0/52754]\n",
      "loss: 10.873621  [ 3200/52754]\n",
      "loss: 9.634498  [ 6400/52754]\n",
      "loss: 13.087108  [ 9600/52754]\n",
      "loss: 9.977947  [12800/52754]\n",
      "loss: 10.016467  [16000/52754]\n",
      "loss: 8.524140  [19200/52754]\n",
      "loss: 10.129745  [22400/52754]\n",
      "loss: 9.908947  [25600/52754]\n",
      "loss: 10.894643  [28800/52754]\n",
      "loss: 8.819798  [32000/52754]\n",
      "loss: 8.704895  [35200/52754]\n",
      "loss: 9.681180  [38400/52754]\n",
      "loss: 7.448075  [41600/52754]\n",
      "loss: 6.696349  [44800/52754]\n",
      "loss: 6.481501  [48000/52754]\n",
      "loss: 9.104568  [51200/52754]\n",
      "Avg loss: 3.844780 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 9.828913  [    0/52754]\n",
      "loss: 6.213482  [ 3200/52754]\n",
      "loss: 8.057502  [ 6400/52754]\n",
      "loss: 8.200226  [ 9600/52754]\n",
      "loss: 8.192598  [12800/52754]\n",
      "loss: 6.325162  [16000/52754]\n",
      "loss: 9.378900  [19200/52754]\n",
      "loss: 7.884407  [22400/52754]\n",
      "loss: 7.395370  [25600/52754]\n",
      "loss: 7.948637  [28800/52754]\n",
      "loss: 10.224874  [32000/52754]\n",
      "loss: 6.971095  [35200/52754]\n",
      "loss: 8.872394  [38400/52754]\n",
      "loss: 6.874991  [41600/52754]\n",
      "loss: 5.980395  [44800/52754]\n",
      "loss: 6.780159  [48000/52754]\n",
      "loss: 6.144532  [51200/52754]\n",
      "Avg loss: 2.787854 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 7.054748  [    0/52754]\n",
      "loss: 6.465740  [ 3200/52754]\n",
      "loss: 8.270698  [ 6400/52754]\n",
      "loss: 9.747317  [ 9600/52754]\n",
      "loss: 7.206669  [12800/52754]\n",
      "loss: 5.920450  [16000/52754]\n",
      "loss: 8.144896  [19200/52754]\n",
      "loss: 10.732325  [22400/52754]\n",
      "loss: 9.278980  [25600/52754]\n",
      "loss: 7.739334  [28800/52754]\n",
      "loss: 8.163149  [32000/52754]\n",
      "loss: 8.106690  [35200/52754]\n",
      "loss: 7.270278  [38400/52754]\n",
      "loss: 5.985585  [41600/52754]\n",
      "loss: 8.427764  [44800/52754]\n",
      "loss: 7.559471  [48000/52754]\n",
      "loss: 5.217863  [51200/52754]\n",
      "Avg loss: 2.908784 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 7.418375  [    0/52754]\n",
      "loss: 7.582591  [ 3200/52754]\n",
      "loss: 7.425373  [ 6400/52754]\n",
      "loss: 8.338886  [ 9600/52754]\n",
      "loss: 5.813463  [12800/52754]\n",
      "loss: 6.394083  [16000/52754]\n",
      "loss: 8.107017  [19200/52754]\n",
      "loss: 6.420181  [22400/52754]\n",
      "loss: 5.791877  [25600/52754]\n",
      "loss: 5.240182  [28800/52754]\n",
      "loss: 8.239429  [32000/52754]\n",
      "loss: 8.512355  [35200/52754]\n",
      "loss: 6.372008  [38400/52754]\n",
      "loss: 7.454256  [41600/52754]\n",
      "loss: 5.748680  [44800/52754]\n",
      "loss: 9.848454  [48000/52754]\n",
      "loss: 8.098873  [51200/52754]\n",
      "Avg loss: 2.752901 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 6.701213  [    0/52754]\n",
      "loss: 7.617129  [ 3200/52754]\n",
      "loss: 6.237839  [ 6400/52754]\n",
      "loss: 5.801116  [ 9600/52754]\n",
      "loss: 7.815965  [12800/52754]\n",
      "loss: 6.018540  [16000/52754]\n",
      "loss: 6.382524  [19200/52754]\n",
      "loss: 7.552037  [22400/52754]\n",
      "loss: 6.525429  [25600/52754]\n",
      "loss: 6.810532  [28800/52754]\n",
      "loss: 7.330551  [32000/52754]\n",
      "loss: 7.359795  [35200/52754]\n",
      "loss: 8.260919  [38400/52754]\n",
      "loss: 5.920411  [41600/52754]\n",
      "loss: 8.210194  [44800/52754]\n",
      "loss: 6.313333  [48000/52754]\n",
      "loss: 5.779637  [51200/52754]\n",
      "Avg loss: 2.755466 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 6.561059  [    0/52754]\n",
      "loss: 6.071705  [ 3200/52754]\n",
      "loss: 5.868024  [ 6400/52754]\n",
      "loss: 6.695264  [ 9600/52754]\n",
      "loss: 7.129600  [12800/52754]\n",
      "loss: 6.878187  [16000/52754]\n",
      "loss: 6.315008  [19200/52754]\n",
      "loss: 6.138516  [22400/52754]\n",
      "loss: 5.919474  [25600/52754]\n",
      "loss: 7.744956  [28800/52754]\n",
      "loss: 7.293067  [32000/52754]\n",
      "loss: 6.221564  [35200/52754]\n",
      "loss: 6.101126  [38400/52754]\n",
      "loss: 6.502145  [41600/52754]\n",
      "loss: 7.072768  [44800/52754]\n",
      "loss: 6.369375  [48000/52754]\n",
      "loss: 7.244398  [51200/52754]\n",
      "Avg loss: 2.760247 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 6.394740  [    0/52754]\n",
      "loss: 6.952616  [ 3200/52754]\n",
      "loss: 6.641739  [ 6400/52754]\n",
      "loss: 5.457511  [ 9600/52754]\n",
      "loss: 6.890223  [12800/52754]\n",
      "loss: 7.152497  [16000/52754]\n",
      "loss: 7.080838  [19200/52754]\n",
      "loss: 6.779238  [22400/52754]\n",
      "loss: 8.318957  [25600/52754]\n",
      "loss: 8.049024  [28800/52754]\n",
      "loss: 6.687492  [32000/52754]\n",
      "loss: 6.746307  [35200/52754]\n",
      "loss: 7.564650  [38400/52754]\n",
      "loss: 6.474869  [41600/52754]\n",
      "loss: 4.501155  [44800/52754]\n",
      "loss: 5.235725  [48000/52754]\n",
      "loss: 3.439645  [51200/52754]\n",
      "Avg loss: 0.329766 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 3.664488  [    0/52754]\n",
      "loss: 4.908170  [ 3200/52754]\n",
      "loss: 4.040208  [ 6400/52754]\n",
      "loss: 3.659836  [ 9600/52754]\n",
      "loss: 3.036992  [12800/52754]\n",
      "loss: 3.369869  [16000/52754]\n",
      "loss: 4.071442  [19200/52754]\n",
      "loss: 4.680699  [22400/52754]\n",
      "loss: 4.767613  [25600/52754]\n",
      "loss: 3.006755  [28800/52754]\n",
      "loss: 3.343086  [32000/52754]\n",
      "loss: 6.500403  [35200/52754]\n",
      "loss: 4.876259  [38400/52754]\n",
      "loss: 2.363854  [41600/52754]\n",
      "loss: 2.744976  [44800/52754]\n",
      "loss: 4.946967  [48000/52754]\n",
      "loss: 5.342854  [51200/52754]\n",
      "Avg loss: 0.292894 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 3.413476  [    0/52754]\n",
      "loss: 4.151807  [ 3200/52754]\n",
      "loss: 4.458033  [ 6400/52754]\n",
      "loss: 3.980090  [ 9600/52754]\n",
      "loss: 2.353644  [12800/52754]\n",
      "loss: 4.529251  [16000/52754]\n",
      "loss: 2.341395  [19200/52754]\n",
      "loss: 5.646753  [22400/52754]\n",
      "loss: 4.692307  [25600/52754]\n",
      "loss: 4.379852  [28800/52754]\n",
      "loss: 4.769629  [32000/52754]\n",
      "loss: 5.463948  [35200/52754]\n",
      "loss: 3.744401  [38400/52754]\n",
      "loss: 2.335950  [41600/52754]\n",
      "loss: 3.616477  [44800/52754]\n",
      "loss: 2.378452  [48000/52754]\n",
      "loss: 4.661893  [51200/52754]\n",
      "Avg loss: 0.283244 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 3.437553  [    0/52754]\n",
      "loss: 3.387670  [ 3200/52754]\n",
      "loss: 3.451417  [ 6400/52754]\n",
      "loss: 4.370360  [ 9600/52754]\n",
      "loss: 5.280854  [12800/52754]\n",
      "loss: 3.637561  [16000/52754]\n",
      "loss: 4.634839  [19200/52754]\n",
      "loss: 3.680029  [22400/52754]\n",
      "loss: 4.244673  [25600/52754]\n",
      "loss: 4.191668  [28800/52754]\n",
      "loss: 5.238092  [32000/52754]\n",
      "loss: 4.877289  [35200/52754]\n",
      "loss: 4.629744  [38400/52754]\n",
      "loss: 3.480153  [41600/52754]\n",
      "loss: 4.093409  [44800/52754]\n",
      "loss: 5.831980  [48000/52754]\n",
      "loss: 3.186332  [51200/52754]\n",
      "Avg loss: 0.240909 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 3.872111  [    0/52754]\n",
      "loss: 4.160227  [ 3200/52754]\n",
      "loss: 3.799319  [ 6400/52754]\n",
      "loss: 4.173396  [ 9600/52754]\n",
      "loss: 3.262348  [12800/52754]\n",
      "loss: 2.858883  [16000/52754]\n",
      "loss: 2.543590  [19200/52754]\n",
      "loss: 5.256188  [22400/52754]\n",
      "loss: 5.249516  [25600/52754]\n",
      "loss: 3.664094  [28800/52754]\n",
      "loss: 3.365910  [32000/52754]\n",
      "loss: 5.441926  [35200/52754]\n",
      "loss: 3.145241  [38400/52754]\n",
      "loss: 4.061570  [41600/52754]\n",
      "loss: 5.541409  [44800/52754]\n",
      "loss: 4.201634  [48000/52754]\n",
      "loss: 3.830537  [51200/52754]\n",
      "Avg loss: 0.245937 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 5.039622  [    0/52754]\n",
      "loss: 3.179297  [ 3200/52754]\n",
      "loss: 3.631030  [ 6400/52754]\n",
      "loss: 5.066054  [ 9600/52754]\n",
      "loss: 3.005076  [12800/52754]\n",
      "loss: 2.523132  [16000/52754]\n",
      "loss: 4.097749  [19200/52754]\n",
      "loss: 5.272356  [22400/52754]\n",
      "loss: 3.968784  [25600/52754]\n",
      "loss: 3.306437  [28800/52754]\n",
      "loss: 4.117478  [32000/52754]\n",
      "loss: 5.862757  [35200/52754]\n",
      "loss: 6.982071  [38400/52754]\n",
      "loss: 3.511519  [41600/52754]\n",
      "loss: 3.653684  [44800/52754]\n",
      "loss: 2.187617  [48000/52754]\n",
      "loss: 3.941746  [51200/52754]\n",
      "Avg loss: 0.224624 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 4.239480  [    0/52754]\n",
      "loss: 6.548857  [ 3200/52754]\n",
      "loss: 2.932700  [ 6400/52754]\n",
      "loss: 2.988226  [ 9600/52754]\n",
      "loss: 2.848354  [12800/52754]\n",
      "loss: 2.968200  [16000/52754]\n",
      "loss: 3.595836  [19200/52754]\n",
      "loss: 3.080983  [22400/52754]\n",
      "loss: 3.806773  [25600/52754]\n",
      "loss: 4.173924  [28800/52754]\n",
      "loss: 4.377417  [32000/52754]\n",
      "loss: 6.323905  [35200/52754]\n",
      "loss: 3.783455  [38400/52754]\n",
      "loss: 3.922078  [41600/52754]\n",
      "loss: 3.519938  [44800/52754]\n",
      "loss: 2.038904  [48000/52754]\n",
      "loss: 4.219727  [51200/52754]\n",
      "Avg loss: 0.257484 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 4.865491  [    0/52754]\n",
      "loss: 6.120820  [ 3200/52754]\n",
      "loss: 4.192814  [ 6400/52754]\n",
      "loss: 4.546898  [ 9600/52754]\n",
      "loss: 3.696130  [12800/52754]\n",
      "loss: 3.414688  [16000/52754]\n",
      "loss: 3.030763  [19200/52754]\n",
      "loss: 3.510104  [22400/52754]\n",
      "loss: 3.007497  [25600/52754]\n",
      "loss: 4.494743  [28800/52754]\n",
      "loss: 5.451782  [32000/52754]\n",
      "loss: 4.396355  [35200/52754]\n",
      "loss: 4.845178  [38400/52754]\n",
      "loss: 2.903038  [41600/52754]\n",
      "loss: 3.324836  [44800/52754]\n",
      "loss: 1.766770  [48000/52754]\n",
      "loss: 2.928518  [51200/52754]\n",
      "Avg loss: 0.257271 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 5.296585  [    0/52754]\n",
      "loss: 3.401018  [ 3200/52754]\n",
      "loss: 3.693200  [ 6400/52754]\n",
      "loss: 4.032984  [ 9600/52754]\n",
      "loss: 3.205613  [12800/52754]\n",
      "loss: 3.095538  [16000/52754]\n",
      "loss: 3.585120  [19200/52754]\n",
      "loss: 3.419789  [22400/52754]\n",
      "loss: 3.864645  [25600/52754]\n",
      "loss: 2.549291  [28800/52754]\n",
      "loss: 3.652507  [32000/52754]\n",
      "loss: 4.081557  [35200/52754]\n",
      "loss: 3.935345  [38400/52754]\n",
      "loss: 3.931834  [41600/52754]\n",
      "loss: 5.638833  [44800/52754]\n",
      "loss: 3.106950  [48000/52754]\n",
      "loss: 3.225737  [51200/52754]\n",
      "Avg loss: 0.298659 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 4.386398  [    0/52754]\n",
      "loss: 4.198508  [ 3200/52754]\n",
      "loss: 3.643284  [ 6400/52754]\n",
      "loss: 2.513691  [ 9600/52754]\n",
      "loss: 3.556273  [12800/52754]\n",
      "loss: 4.048401  [16000/52754]\n",
      "loss: 3.599679  [19200/52754]\n",
      "loss: 3.806903  [22400/52754]\n",
      "loss: 3.623736  [25600/52754]\n",
      "loss: 2.708117  [28800/52754]\n",
      "loss: 2.758557  [32000/52754]\n",
      "loss: 4.048684  [35200/52754]\n",
      "loss: 3.013996  [38400/52754]\n",
      "loss: 2.864432  [41600/52754]\n",
      "loss: 3.284963  [44800/52754]\n",
      "loss: 4.371707  [48000/52754]\n",
      "loss: 3.915690  [51200/52754]\n",
      "Avg loss: 0.234517 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 3.826098  [    0/52754]\n",
      "loss: 4.173894  [ 3200/52754]\n",
      "loss: 2.701642  [ 6400/52754]\n",
      "loss: 3.461347  [ 9600/52754]\n",
      "loss: 2.291875  [12800/52754]\n",
      "loss: 4.808182  [16000/52754]\n",
      "loss: 6.463754  [19200/52754]\n",
      "loss: 4.991762  [22400/52754]\n",
      "loss: 4.321337  [25600/52754]\n",
      "loss: 4.749752  [28800/52754]\n",
      "loss: 3.672384  [32000/52754]\n",
      "loss: 5.640124  [35200/52754]\n",
      "loss: 4.083766  [38400/52754]\n",
      "loss: 4.541708  [41600/52754]\n",
      "loss: 2.545457  [44800/52754]\n",
      "loss: 2.899177  [48000/52754]\n",
      "loss: 4.226389  [51200/52754]\n",
      "Avg loss: 0.228656 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 3.598400  [    0/52754]\n",
      "loss: 4.499988  [ 3200/52754]\n",
      "loss: 3.238732  [ 6400/52754]\n",
      "loss: 5.093489  [ 9600/52754]\n",
      "loss: 3.690106  [12800/52754]\n",
      "loss: 2.993313  [16000/52754]\n",
      "loss: 3.777980  [19200/52754]\n",
      "loss: 3.045600  [22400/52754]\n",
      "loss: 3.082983  [25600/52754]\n",
      "loss: 3.609396  [28800/52754]\n",
      "loss: 4.535536  [32000/52754]\n",
      "loss: 2.322628  [35200/52754]\n",
      "loss: 2.989623  [38400/52754]\n",
      "loss: 3.403145  [41600/52754]\n",
      "loss: 3.851477  [44800/52754]\n",
      "loss: 4.321101  [48000/52754]\n",
      "loss: 2.591713  [51200/52754]\n",
      "Avg loss: 0.265727 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 3.610817  [    0/52754]\n",
      "loss: 2.484921  [ 3200/52754]\n",
      "loss: 3.328598  [ 6400/52754]\n",
      "loss: 3.329607  [ 9600/52754]\n",
      "loss: 4.815413  [12800/52754]\n",
      "loss: 4.719858  [16000/52754]\n",
      "loss: 5.752544  [19200/52754]\n",
      "loss: 3.682952  [22400/52754]\n",
      "loss: 2.537880  [25600/52754]\n",
      "loss: 5.572980  [28800/52754]\n",
      "loss: 4.789219  [32000/52754]\n",
      "loss: 2.988570  [35200/52754]\n",
      "loss: 3.295734  [38400/52754]\n",
      "loss: 5.040957  [41600/52754]\n",
      "loss: 3.719641  [44800/52754]\n",
      "loss: 3.578366  [48000/52754]\n",
      "loss: 3.655236  [51200/52754]\n",
      "Avg loss: 0.224386 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 3.160746  [    0/52754]\n",
      "loss: 3.603296  [ 3200/52754]\n",
      "loss: 4.936577  [ 6400/52754]\n",
      "loss: 3.249203  [ 9600/52754]\n",
      "loss: 5.796415  [12800/52754]\n",
      "loss: 3.636784  [16000/52754]\n",
      "loss: 3.810283  [19200/52754]\n",
      "loss: 4.745083  [22400/52754]\n",
      "loss: 3.532459  [25600/52754]\n",
      "loss: 3.162968  [28800/52754]\n",
      "loss: 3.659309  [32000/52754]\n",
      "loss: 3.440136  [35200/52754]\n",
      "loss: 3.040046  [38400/52754]\n",
      "loss: 3.948872  [41600/52754]\n",
      "loss: 3.234195  [44800/52754]\n",
      "loss: 2.593908  [48000/52754]\n",
      "loss: 4.007293  [51200/52754]\n",
      "Avg loss: 0.224468 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 3.982826  [    0/52754]\n",
      "loss: 3.965144  [ 3200/52754]\n",
      "loss: 3.369891  [ 6400/52754]\n",
      "loss: 3.276509  [ 9600/52754]\n",
      "loss: 3.801151  [12800/52754]\n",
      "loss: 4.431709  [16000/52754]\n",
      "loss: 4.821431  [19200/52754]\n",
      "loss: 4.134712  [22400/52754]\n",
      "loss: 3.589663  [25600/52754]\n",
      "loss: 2.364709  [28800/52754]\n",
      "loss: 3.376252  [32000/52754]\n",
      "loss: 2.366827  [35200/52754]\n",
      "loss: 4.957135  [38400/52754]\n",
      "loss: 3.606972  [41600/52754]\n",
      "loss: 3.737376  [44800/52754]\n",
      "loss: 2.524925  [48000/52754]\n",
      "loss: 2.329355  [51200/52754]\n",
      "Avg loss: 0.225913 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.852425  [    0/52754]\n",
      "loss: 4.349360  [ 3200/52754]\n",
      "loss: 2.998446  [ 6400/52754]\n",
      "loss: 4.591074  [ 9600/52754]\n",
      "loss: 3.770390  [12800/52754]\n",
      "loss: 3.016279  [16000/52754]\n",
      "loss: 2.492425  [19200/52754]\n",
      "loss: 3.711050  [22400/52754]\n",
      "loss: 3.656122  [25600/52754]\n",
      "loss: 3.649644  [28800/52754]\n",
      "loss: 2.440196  [32000/52754]\n",
      "loss: 3.983983  [35200/52754]\n",
      "loss: 2.843232  [38400/52754]\n",
      "loss: 2.623787  [41600/52754]\n",
      "loss: 2.751798  [44800/52754]\n",
      "loss: 3.552227  [48000/52754]\n",
      "loss: 3.051944  [51200/52754]\n",
      "Avg loss: 0.239565 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 3.909211  [    0/52754]\n",
      "loss: 4.246106  [ 3200/52754]\n",
      "loss: 2.339743  [ 6400/52754]\n",
      "loss: 3.035106  [ 9600/52754]\n",
      "loss: 2.702502  [12800/52754]\n",
      "loss: 3.432766  [16000/52754]\n",
      "loss: 4.298863  [19200/52754]\n",
      "loss: 1.927104  [22400/52754]\n",
      "loss: 3.040135  [25600/52754]\n",
      "loss: 2.892494  [28800/52754]\n",
      "loss: 3.341285  [32000/52754]\n",
      "loss: 4.575235  [35200/52754]\n",
      "loss: 4.719048  [38400/52754]\n",
      "loss: 4.508245  [41600/52754]\n",
      "loss: 4.566533  [44800/52754]\n",
      "loss: 4.486421  [48000/52754]\n",
      "loss: 4.280584  [51200/52754]\n",
      "Avg loss: 0.255393 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.183330  [    0/52754]\n",
      "loss: 3.097687  [ 3200/52754]\n",
      "loss: 4.861108  [ 6400/52754]\n",
      "loss: 3.147380  [ 9600/52754]\n",
      "loss: 4.073973  [12800/52754]\n",
      "loss: 3.784426  [16000/52754]\n",
      "loss: 2.744507  [19200/52754]\n",
      "loss: 3.969119  [22400/52754]\n",
      "loss: 3.077120  [25600/52754]\n",
      "loss: 2.978762  [28800/52754]\n",
      "loss: 3.686442  [32000/52754]\n",
      "loss: 3.772853  [35200/52754]\n",
      "loss: 3.026122  [38400/52754]\n",
      "loss: 4.932653  [41600/52754]\n",
      "loss: 2.537010  [44800/52754]\n",
      "loss: 3.357838  [48000/52754]\n",
      "loss: 3.842909  [51200/52754]\n",
      "Avg loss: 0.254263 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 3.091630  [    0/52754]\n",
      "loss: 3.672709  [ 3200/52754]\n",
      "loss: 4.228429  [ 6400/52754]\n",
      "loss: 3.147242  [ 9600/52754]\n",
      "loss: 2.782591  [12800/52754]\n",
      "loss: 2.053110  [16000/52754]\n",
      "loss: 3.310546  [19200/52754]\n",
      "loss: 2.917816  [22400/52754]\n",
      "loss: 2.898209  [25600/52754]\n",
      "loss: 4.325430  [28800/52754]\n",
      "loss: 4.423173  [32000/52754]\n",
      "loss: 2.917114  [35200/52754]\n",
      "loss: 2.250664  [38400/52754]\n",
      "loss: 3.717373  [41600/52754]\n",
      "loss: 3.252337  [44800/52754]\n",
      "loss: 4.325317  [48000/52754]\n",
      "loss: 3.323101  [51200/52754]\n",
      "Avg loss: 0.255825 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.833473  [    0/52754]\n",
      "loss: 3.764573  [ 3200/52754]\n",
      "loss: 3.032487  [ 6400/52754]\n",
      "loss: 3.851260  [ 9600/52754]\n",
      "loss: 4.346243  [12800/52754]\n",
      "loss: 4.010672  [16000/52754]\n",
      "loss: 3.060102  [19200/52754]\n",
      "loss: 2.274652  [22400/52754]\n",
      "loss: 3.828710  [25600/52754]\n",
      "loss: 4.077048  [28800/52754]\n",
      "loss: 2.462870  [32000/52754]\n",
      "loss: 4.098950  [35200/52754]\n",
      "loss: 2.752816  [38400/52754]\n",
      "loss: 4.037943  [41600/52754]\n",
      "loss: 3.810362  [44800/52754]\n",
      "loss: 3.363395  [48000/52754]\n",
      "loss: 3.929864  [51200/52754]\n",
      "Avg loss: 0.257277 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 3.272883  [    0/52754]\n",
      "loss: 2.836887  [ 3200/52754]\n",
      "loss: 4.325636  [ 6400/52754]\n",
      "loss: 2.412480  [ 9600/52754]\n",
      "loss: 3.840421  [12800/52754]\n",
      "loss: 2.977957  [16000/52754]\n",
      "loss: 3.418875  [19200/52754]\n",
      "loss: 2.742665  [22400/52754]\n",
      "loss: 3.397433  [25600/52754]\n",
      "loss: 3.252129  [28800/52754]\n",
      "loss: 3.287831  [32000/52754]\n",
      "loss: 3.656139  [35200/52754]\n",
      "loss: 2.985724  [38400/52754]\n",
      "loss: 5.843272  [41600/52754]\n",
      "loss: 3.155823  [44800/52754]\n",
      "loss: 3.544641  [48000/52754]\n",
      "loss: 3.500420  [51200/52754]\n",
      "Avg loss: 0.253166 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 4.660166  [    0/52754]\n",
      "loss: 4.375876  [ 3200/52754]\n",
      "loss: 1.935876  [ 6400/52754]\n",
      "loss: 2.523399  [ 9600/52754]\n",
      "loss: 2.465759  [12800/52754]\n",
      "loss: 3.102676  [16000/52754]\n",
      "loss: 5.134851  [19200/52754]\n",
      "loss: 2.525951  [22400/52754]\n",
      "loss: 2.717693  [25600/52754]\n",
      "loss: 4.311749  [28800/52754]\n",
      "loss: 3.542080  [32000/52754]\n",
      "loss: 2.296372  [35200/52754]\n",
      "loss: 3.249420  [38400/52754]\n",
      "loss: 1.799536  [41600/52754]\n",
      "loss: 4.143960  [44800/52754]\n",
      "loss: 3.463869  [48000/52754]\n",
      "loss: 3.128719  [51200/52754]\n",
      "Avg loss: 0.254263 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.370940  [    0/52754]\n",
      "loss: 2.319209  [ 3200/52754]\n",
      "loss: 3.755095  [ 6400/52754]\n",
      "loss: 3.910112  [ 9600/52754]\n",
      "loss: 2.993974  [12800/52754]\n",
      "loss: 3.402055  [16000/52754]\n",
      "loss: 3.039519  [19200/52754]\n",
      "loss: 2.852056  [22400/52754]\n",
      "loss: 3.377101  [25600/52754]\n",
      "loss: 5.002313  [28800/52754]\n",
      "loss: 4.704097  [32000/52754]\n",
      "loss: 3.773325  [35200/52754]\n",
      "loss: 5.115072  [38400/52754]\n",
      "loss: 4.382881  [41600/52754]\n",
      "loss: 3.625651  [44800/52754]\n",
      "loss: 4.961644  [48000/52754]\n",
      "loss: 3.745826  [51200/52754]\n",
      "Avg loss: 0.228607 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 5.596423  [    0/52754]\n",
      "loss: 4.072536  [ 3200/52754]\n",
      "loss: 2.879619  [ 6400/52754]\n",
      "loss: 5.632260  [ 9600/52754]\n",
      "loss: 2.265893  [12800/52754]\n",
      "loss: 3.895160  [16000/52754]\n",
      "loss: 3.162297  [19200/52754]\n",
      "loss: 4.128814  [22400/52754]\n",
      "loss: 5.251123  [25600/52754]\n",
      "loss: 5.258633  [28800/52754]\n",
      "loss: 2.968416  [32000/52754]\n",
      "loss: 3.005975  [35200/52754]\n",
      "loss: 3.538128  [38400/52754]\n",
      "loss: 2.029983  [41600/52754]\n",
      "loss: 3.924283  [44800/52754]\n",
      "loss: 2.102219  [48000/52754]\n",
      "loss: 3.020404  [51200/52754]\n",
      "Avg loss: 0.230697 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 3.806081  [    0/52754]\n",
      "loss: 3.317732  [ 3200/52754]\n",
      "loss: 3.253413  [ 6400/52754]\n",
      "loss: 3.447613  [ 9600/52754]\n",
      "loss: 3.152865  [12800/52754]\n",
      "loss: 3.949369  [16000/52754]\n",
      "loss: 3.245082  [19200/52754]\n",
      "loss: 2.627003  [22400/52754]\n",
      "loss: 5.456026  [25600/52754]\n",
      "loss: 3.965307  [28800/52754]\n",
      "loss: 4.084462  [32000/52754]\n",
      "loss: 3.035482  [35200/52754]\n",
      "loss: 2.805367  [38400/52754]\n",
      "loss: 3.341044  [41600/52754]\n",
      "loss: 4.472692  [44800/52754]\n",
      "loss: 3.152016  [48000/52754]\n",
      "loss: 3.912050  [51200/52754]\n",
      "Avg loss: 0.180622 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 5.791375  [    0/52754]\n",
      "loss: 2.141278  [ 3200/52754]\n",
      "loss: 3.684609  [ 6400/52754]\n",
      "loss: 2.780571  [ 9600/52754]\n",
      "loss: 3.945071  [12800/52754]\n",
      "loss: 3.871906  [16000/52754]\n",
      "loss: 3.123028  [19200/52754]\n",
      "loss: 3.941432  [22400/52754]\n",
      "loss: 2.795197  [25600/52754]\n",
      "loss: 3.803641  [28800/52754]\n",
      "loss: 2.888781  [32000/52754]\n",
      "loss: 4.631903  [35200/52754]\n",
      "loss: 3.331956  [38400/52754]\n",
      "loss: 4.444492  [41600/52754]\n",
      "loss: 4.981439  [44800/52754]\n",
      "loss: 3.416750  [48000/52754]\n",
      "loss: 3.722669  [51200/52754]\n",
      "Avg loss: 0.159702 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 3.229823  [    0/52754]\n",
      "loss: 3.944622  [ 3200/52754]\n",
      "loss: 4.186921  [ 6400/52754]\n",
      "loss: 3.000503  [ 9600/52754]\n",
      "loss: 3.226719  [12800/52754]\n",
      "loss: 3.031318  [16000/52754]\n",
      "loss: 3.730989  [19200/52754]\n",
      "loss: 4.240965  [22400/52754]\n",
      "loss: 3.644341  [25600/52754]\n",
      "loss: 3.573429  [28800/52754]\n",
      "loss: 3.258946  [32000/52754]\n",
      "loss: 2.612426  [35200/52754]\n",
      "loss: 3.446788  [38400/52754]\n",
      "loss: 2.738650  [41600/52754]\n",
      "loss: 3.864801  [44800/52754]\n",
      "loss: 8.360323  [48000/52754]\n",
      "loss: 3.886035  [51200/52754]\n",
      "Avg loss: 0.176639 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.516781  [    0/52754]\n",
      "loss: 3.486993  [ 3200/52754]\n",
      "loss: 4.449627  [ 6400/52754]\n",
      "loss: 2.932152  [ 9600/52754]\n",
      "loss: 2.884737  [12800/52754]\n",
      "loss: 4.341924  [16000/52754]\n",
      "loss: 3.822097  [19200/52754]\n",
      "loss: 2.520186  [22400/52754]\n",
      "loss: 2.243380  [25600/52754]\n",
      "loss: 4.973966  [28800/52754]\n",
      "loss: 4.473649  [32000/52754]\n",
      "loss: 3.838193  [35200/52754]\n",
      "loss: 2.756032  [38400/52754]\n",
      "loss: 3.986363  [41600/52754]\n",
      "loss: 3.603676  [44800/52754]\n",
      "loss: 3.400498  [48000/52754]\n",
      "loss: 1.887894  [51200/52754]\n",
      "Avg loss: 0.205685 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 3.043378  [    0/52754]\n",
      "loss: 2.296621  [ 3200/52754]\n",
      "loss: 3.281753  [ 6400/52754]\n",
      "loss: 2.794819  [ 9600/52754]\n",
      "loss: 2.694649  [12800/52754]\n",
      "loss: 3.267346  [16000/52754]\n",
      "loss: 3.319380  [19200/52754]\n",
      "loss: 4.532475  [22400/52754]\n",
      "loss: 3.384684  [25600/52754]\n",
      "loss: 2.812503  [28800/52754]\n",
      "loss: 3.530036  [32000/52754]\n",
      "loss: 2.722788  [35200/52754]\n",
      "loss: 4.442939  [38400/52754]\n",
      "loss: 2.859652  [41600/52754]\n",
      "loss: 4.649696  [44800/52754]\n",
      "loss: 2.386387  [48000/52754]\n",
      "loss: 3.145875  [51200/52754]\n",
      "Avg loss: 0.173413 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 3.249199  [    0/52754]\n",
      "loss: 2.937741  [ 3200/52754]\n",
      "loss: 4.057973  [ 6400/52754]\n",
      "loss: 2.679765  [ 9600/52754]\n",
      "loss: 3.197223  [12800/52754]\n",
      "loss: 4.549826  [16000/52754]\n",
      "loss: 2.807528  [19200/52754]\n",
      "loss: 2.256356  [22400/52754]\n",
      "loss: 3.360282  [25600/52754]\n",
      "loss: 3.750226  [28800/52754]\n",
      "loss: 4.281177  [32000/52754]\n",
      "loss: 4.778099  [35200/52754]\n",
      "loss: 2.000488  [38400/52754]\n",
      "loss: 3.292174  [41600/52754]\n",
      "loss: 2.513505  [44800/52754]\n",
      "loss: 2.700118  [48000/52754]\n",
      "loss: 3.814739  [51200/52754]\n",
      "Avg loss: 0.178416 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 4.388141  [    0/52754]\n",
      "loss: 4.205885  [ 3200/52754]\n",
      "loss: 2.886029  [ 6400/52754]\n",
      "loss: 3.270586  [ 9600/52754]\n",
      "loss: 4.495147  [12800/52754]\n",
      "loss: 3.372246  [16000/52754]\n",
      "loss: 3.110979  [19200/52754]\n",
      "loss: 2.506680  [22400/52754]\n",
      "loss: 2.696974  [25600/52754]\n",
      "loss: 2.810559  [28800/52754]\n",
      "loss: 3.354134  [32000/52754]\n",
      "loss: 4.037945  [35200/52754]\n",
      "loss: 3.520744  [38400/52754]\n",
      "loss: 3.870237  [41600/52754]\n",
      "loss: 3.238239  [44800/52754]\n",
      "loss: 3.332847  [48000/52754]\n",
      "loss: 4.436701  [51200/52754]\n",
      "Avg loss: 0.168239 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 4.295760  [    0/52754]\n",
      "loss: 2.910674  [ 3200/52754]\n",
      "loss: 2.859837  [ 6400/52754]\n",
      "loss: 3.841234  [ 9600/52754]\n",
      "loss: 3.092005  [12800/52754]\n",
      "loss: 4.099390  [16000/52754]\n",
      "loss: 3.022629  [19200/52754]\n",
      "loss: 2.527104  [22400/52754]\n",
      "loss: 2.711055  [25600/52754]\n",
      "loss: 3.130970  [28800/52754]\n",
      "loss: 3.641966  [32000/52754]\n",
      "loss: 4.020170  [35200/52754]\n",
      "loss: 2.103376  [38400/52754]\n",
      "loss: 3.895557  [41600/52754]\n",
      "loss: 2.291600  [44800/52754]\n",
      "loss: 3.384767  [48000/52754]\n",
      "loss: 4.600595  [51200/52754]\n",
      "Avg loss: 0.165896 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.943011  [    0/52754]\n",
      "loss: 3.488738  [ 3200/52754]\n",
      "loss: 3.324121  [ 6400/52754]\n",
      "loss: 2.790719  [ 9600/52754]\n",
      "loss: 2.158464  [12800/52754]\n",
      "loss: 3.375063  [16000/52754]\n",
      "loss: 3.175486  [19200/52754]\n",
      "loss: 3.011725  [22400/52754]\n",
      "loss: 4.013682  [25600/52754]\n",
      "loss: 2.476025  [28800/52754]\n",
      "loss: 2.088172  [32000/52754]\n",
      "loss: 2.735817  [35200/52754]\n",
      "loss: 4.037506  [38400/52754]\n",
      "loss: 3.394621  [41600/52754]\n",
      "loss: 3.000050  [44800/52754]\n",
      "loss: 5.599467  [48000/52754]\n",
      "loss: 2.962779  [51200/52754]\n",
      "Avg loss: 0.215007 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.803428  [    0/52754]\n",
      "loss: 3.441734  [ 3200/52754]\n",
      "loss: 3.078802  [ 6400/52754]\n",
      "loss: 3.427337  [ 9600/52754]\n",
      "loss: 2.698631  [12800/52754]\n",
      "loss: 3.024670  [16000/52754]\n",
      "loss: 3.633398  [19200/52754]\n",
      "loss: 4.302446  [22400/52754]\n",
      "loss: 2.498383  [25600/52754]\n",
      "loss: 2.346062  [28800/52754]\n",
      "loss: 4.378563  [32000/52754]\n",
      "loss: 3.995106  [35200/52754]\n",
      "loss: 2.801354  [38400/52754]\n",
      "loss: 2.277154  [41600/52754]\n",
      "loss: 5.947042  [44800/52754]\n",
      "loss: 2.683676  [48000/52754]\n",
      "loss: 3.577502  [51200/52754]\n",
      "Avg loss: 0.170543 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 4.764406  [    0/52754]\n",
      "loss: 2.762098  [ 3200/52754]\n",
      "loss: 2.687214  [ 6400/52754]\n",
      "loss: 3.327178  [ 9600/52754]\n",
      "loss: 3.953474  [12800/52754]\n",
      "loss: 4.093881  [16000/52754]\n",
      "loss: 2.912370  [19200/52754]\n",
      "loss: 4.918498  [22400/52754]\n",
      "loss: 4.792190  [25600/52754]\n",
      "loss: 2.224465  [28800/52754]\n",
      "loss: 3.162270  [32000/52754]\n",
      "loss: 3.254435  [35200/52754]\n",
      "loss: 3.604820  [38400/52754]\n",
      "loss: 2.602382  [41600/52754]\n",
      "loss: 3.204460  [44800/52754]\n",
      "loss: 3.662364  [48000/52754]\n",
      "loss: 3.544004  [51200/52754]\n",
      "Avg loss: 0.159407 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 3.150692  [    0/52754]\n",
      "loss: 2.735704  [ 3200/52754]\n",
      "loss: 1.888146  [ 6400/52754]\n",
      "loss: 1.753220  [ 9600/52754]\n",
      "loss: 3.651961  [12800/52754]\n",
      "loss: 3.004272  [16000/52754]\n",
      "loss: 3.011173  [19200/52754]\n",
      "loss: 4.012848  [22400/52754]\n",
      "loss: 3.141236  [25600/52754]\n",
      "loss: 3.194565  [28800/52754]\n",
      "loss: 2.680612  [32000/52754]\n",
      "loss: 3.157754  [35200/52754]\n",
      "loss: 4.568076  [38400/52754]\n",
      "loss: 6.557302  [41600/52754]\n",
      "loss: 3.158910  [44800/52754]\n",
      "loss: 3.894405  [48000/52754]\n",
      "loss: 3.637252  [51200/52754]\n",
      "Avg loss: 0.167083 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 3.056365  [    0/52754]\n",
      "loss: 2.825359  [ 3200/52754]\n",
      "loss: 3.253699  [ 6400/52754]\n",
      "loss: 3.102855  [ 9600/52754]\n",
      "loss: 3.454746  [12800/52754]\n",
      "loss: 3.019859  [16000/52754]\n",
      "loss: 3.081691  [19200/52754]\n",
      "loss: 4.433919  [22400/52754]\n",
      "loss: 2.090959  [25600/52754]\n",
      "loss: 3.202931  [28800/52754]\n",
      "loss: 4.300631  [32000/52754]\n",
      "loss: 3.378169  [35200/52754]\n",
      "loss: 3.214392  [38400/52754]\n",
      "loss: 3.759355  [41600/52754]\n",
      "loss: 3.062393  [44800/52754]\n",
      "loss: 2.214362  [48000/52754]\n",
      "loss: 2.964324  [51200/52754]\n",
      "Avg loss: 0.170387 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.917917  [    0/52754]\n",
      "loss: 2.621630  [ 3200/52754]\n",
      "loss: 3.362773  [ 6400/52754]\n",
      "loss: 3.490452  [ 9600/52754]\n",
      "loss: 4.579675  [12800/52754]\n",
      "loss: 3.768803  [16000/52754]\n",
      "loss: 4.158010  [19200/52754]\n",
      "loss: 2.696786  [22400/52754]\n",
      "loss: 3.942783  [25600/52754]\n",
      "loss: 2.955116  [28800/52754]\n",
      "loss: 3.132662  [32000/52754]\n",
      "loss: 3.502663  [35200/52754]\n",
      "loss: 4.147627  [38400/52754]\n",
      "loss: 2.705748  [41600/52754]\n",
      "loss: 3.401380  [44800/52754]\n",
      "loss: 3.389738  [48000/52754]\n",
      "loss: 3.633717  [51200/52754]\n",
      "Avg loss: 0.213204 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 3.401917  [    0/52754]\n",
      "loss: 4.088439  [ 3200/52754]\n",
      "loss: 3.512814  [ 6400/52754]\n",
      "loss: 5.220530  [ 9600/52754]\n",
      "loss: 4.165645  [12800/52754]\n",
      "loss: 2.907250  [16000/52754]\n",
      "loss: 4.302368  [19200/52754]\n",
      "loss: 4.012813  [22400/52754]\n",
      "loss: 3.093190  [25600/52754]\n",
      "loss: 2.385620  [28800/52754]\n",
      "loss: 3.905497  [32000/52754]\n",
      "loss: 2.749971  [35200/52754]\n",
      "loss: 3.140448  [38400/52754]\n",
      "loss: 5.640248  [41600/52754]\n",
      "loss: 5.302534  [44800/52754]\n",
      "loss: 2.208050  [48000/52754]\n",
      "loss: 2.923459  [51200/52754]\n",
      "Avg loss: 0.174342 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 3.552204  [    0/52754]\n",
      "loss: 3.156809  [ 3200/52754]\n",
      "loss: 2.101180  [ 6400/52754]\n",
      "loss: 2.790021  [ 9600/52754]\n",
      "loss: 3.027627  [12800/52754]\n",
      "loss: 3.146822  [16000/52754]\n",
      "loss: 2.604828  [19200/52754]\n",
      "loss: 3.025282  [22400/52754]\n",
      "loss: 4.297145  [25600/52754]\n",
      "loss: 2.351982  [28800/52754]\n",
      "loss: 2.960155  [32000/52754]\n",
      "loss: 2.998479  [35200/52754]\n",
      "loss: 2.231570  [38400/52754]\n",
      "loss: 5.851911  [41600/52754]\n",
      "loss: 3.763186  [44800/52754]\n",
      "loss: 3.067999  [48000/52754]\n",
      "loss: 2.090066  [51200/52754]\n",
      "Avg loss: 0.178924 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(validation_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d643d0a-3915-46ad-a477-e414da935787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving V\n",
    "\n",
    "np.save('data/best_eigenvectors.npy', V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda7b79-220a-47ea-97e8-248e9d73ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# # Print optimizer's state_dict\n",
    "# print(\"Optimizer's state_dict:\")\n",
    "# for var_name in optimizer.state_dict():\n",
    "#     print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af3c7cfc-22bd-4fac-a0b1-34d4c54dbb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'data/autoencoder')\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, 'data/autoencoder_best_spectralx2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d79faf4b-ad8a-4411-b0f2-dd1fd16b4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9648a-249c-4582-aa2e-ff6e2d39a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_numpy_matrix(adj_mat)\n",
    "nx.draw_networkx(G, node_size=20, with_labels=False, linewidths=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e9be4ae-1ffd-415b-b2cd-b51c799d96d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('data/Intra-dataset/Zheng 68K/Labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f19209f-7420-4a50-96a2-8446b22866a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0    1    2    3    4    5    6    7    8    9    10\n",
      "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
      "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
      "2      0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
      "3      0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "4      0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
      "65938  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
      "65939  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
      "65940  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
      "65941  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
      "65942  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
      "\n",
      "[65943 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "labels_df = pd.DataFrame(encoder.fit_transform(labels[['x']]).toarray())\n",
    "\n",
    "print(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b119e1-9a86-4d6d-a961-1f67db70ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_dict = torch.load('data/autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9fac7c9-4a08-4610-8946-1c68c22ec2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skeleton for loading trained spectral encoder\n",
    "\n",
    "class trained_encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, nhidden, p, adj, batch_size, dropout):\n",
    "        super(trained_encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.nhidden = nhidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.poolsize = 8\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.p = p\n",
    "        self.adj = Parameter(torch.from_numpy(adj).contiguous().float(), requires_grad=False).to(self.device)\n",
    "        self.gcn = SpectralConvolution(self.input_dim, self.nhidden, self.batch_size, bias=False)\n",
    "        self.gcn2 = SpectralConvolution(self.input_dim, self.nhidden, self.batch_size, bias=False)\n",
    "        self.encoder_a1 = nn.Linear(int(((self.input_dim//self.p)*nhidden)), 128, bias=True)\n",
    "        self.encoder_a2 = nn.Linear(128, 32, bias=True)\n",
    "        \n",
    "#         self.decoder1 = nn.Linear(32, 128, bias=True)\n",
    "        \n",
    "\n",
    "    def graph_max_pool(self, x, p):\n",
    "        if p > 1:\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x F x V\n",
    "            x = nn.MaxPool1d(p)(x)             # B x F x V/p\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x V/p x F\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #encoder\n",
    "        stream1 = torch.unsqueeze(x, -1)\n",
    "        stream1 = F.relu(self.gcn(stream1, self.adj))\n",
    "        stream1 = F.relu(self.gcn2(stream1, self.adj))\n",
    "        stream1 = self.graph_max_pool(stream1, self.p)\n",
    "        stream1 = torch.flatten(stream1, start_dim = 1)\n",
    "        stream1 = F.relu(self.encoder_a1(stream1))\n",
    "        stream1 = self.dropout(stream1)\n",
    "        outputs = F.relu(self.encoder_a2(stream1))\n",
    "        \n",
    "#         outputs = F.relu(self.decoder1(stream1))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d525df02-f3a2-4ad2-a84b-7d73e9126a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#Trained encoder\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "trained_encoder_model = trained_encoder(pbmc.shape[1], nhidden=32, p=8, adj=V, batch_size=batch_size, dropout=0.3)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2e848f6-3507-4f50-9247-a8fa2295f3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_encoder_model_dict = trained_encoder_model.state_dict()\n",
    "\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in trained_model_dict.items() if k in trained_encoder_model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "trained_encoder_model_dict.update(pretrained_dict) \n",
    "# # 3. load the new state dict\n",
    "trained_encoder_model.load_state_dict(pretrained_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07bba43-c3a4-4741-b367-42e6d7c1becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in trained_encoder_model.state_dict().items():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cc681ae3-c4de-4b2b-a50a-cf37c8e7d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_lst = ['encoder_a1.weight', 'encoder_a2.bias']\n",
    "\n",
    "# for name, param in trained_encoder_model.named_parameters():\n",
    "#     if name not in grad_lst:\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3bc9cccc-2416-400c-9837-e8056223f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_a1.weight\n",
      "encoder_a2.bias\n"
     ]
    }
   ],
   "source": [
    "# for name, param in trained_encoder_model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec94a486-f695-47b9-a912-b88f3bd3265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in trained_encoder_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f928c1d-5473-4515-bb0e-65ef6a25be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class combined_net(nn.Module):\n",
    "    def __init__(self, encoder, input_dim, device):\n",
    "        super(combined_net, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc_input=nn.Linear(input_dim,512)\n",
    "        self.fc1=nn.Linear(512,128)\n",
    "        self.fc2=nn.Linear(128,32)\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "        self.fc3=nn.Linear(64, 11)\n",
    "        self.device=device\n",
    "    \n",
    "    def forward(self,x):\n",
    "        stream1 = self.encoder(x)\n",
    "        \n",
    "        \n",
    "        stream2 = F.relu(self.fc_input(x))\n",
    "        stream2 = F.relu(self.fc1(stream2))\n",
    "        stream2 = self.dropout(stream2)\n",
    "        stream2 = F.relu(self.fc2(stream2))\n",
    "        \n",
    "        combined = torch.cat((stream1, stream2), -1)\n",
    "        outputs = torch.softmax(self.fc3(combined), dim=-1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00b55452-55d8-4033-b126-21228f39e15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "combined_net(\n",
      "  (encoder): trained_encoder(\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (gcn): SpectralConvolution (32)\n",
      "    (gcn2): SpectralConvolution (32)\n",
      "    (encoder_a1): Linear(in_features=3840, out_features=128, bias=True)\n",
      "    (encoder_a2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  )\n",
      "  (fc_input): Linear(in_features=960, out_features=512, bias=True)\n",
      "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc3): Linear(in_features=64, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "combined_model = combined_net(encoder=trained_encoder_model, input_dim=pbmc.shape[1], device=device)\n",
    "print(combined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0782ede1-e70e-4e7e-89be-f8f4461e93be",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(combined_model.parameters(),\n",
    "#                              lr = 1e-3,\n",
    "#                              weight_decay = 1e-8)\n",
    "\n",
    "optimizer = torch.optim.SGD(combined_model.parameters(),\n",
    "                             lr = 1e-2)\n",
    "\n",
    "x_tensor = torch.tensor(pbmc[:len(pbmc)-23].astype(np.float32))\n",
    "y_tensor = torch.tensor(labels_df[:len(pbmc)-23].values.astype(np.float32))\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# batch_size = 32\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 30\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(x_tensor)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "314f667c-3834-41bf-821d-272ee5c5ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = math.floor(len(dataloader.dataset)*0.8)\n",
    "    model.train()\n",
    "    correct = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            correct /= 100*batch_size\n",
    "\n",
    "            print(f\"Train accuracy: {(100*correct):>0.1f}%, loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            correct = 0\n",
    "            \n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = math.floor(len(dataloader.dataset)*0.2)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    print(correct)\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e2fbd-f8d1-47a9-8e57-9fe08b499f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train accuracy: 0.0%, loss: 2.429672  [    0/52736]\n",
      "Train accuracy: 28.0%, loss: 2.324325  [ 3200/52736]\n",
      "Train accuracy: 30.8%, loss: 2.199360  [ 6400/52736]\n",
      "Train accuracy: 30.2%, loss: 2.198833  [ 9600/52736]\n",
      "Train accuracy: 29.6%, loss: 2.199478  [12800/52736]\n",
      "Train accuracy: 31.3%, loss: 2.106249  [16000/52736]\n",
      "Train accuracy: 30.4%, loss: 2.261825  [19200/52736]\n",
      "Train accuracy: 31.6%, loss: 2.199147  [22400/52736]\n",
      "Train accuracy: 32.0%, loss: 2.261720  [25600/52736]\n",
      "Train accuracy: 31.8%, loss: 2.355536  [28800/52736]\n",
      "Train accuracy: 31.2%, loss: 2.230384  [32000/52736]\n",
      "Train accuracy: 31.5%, loss: 2.230618  [35200/52736]\n",
      "Train accuracy: 30.9%, loss: 2.324711  [38400/52736]\n",
      "Train accuracy: 30.0%, loss: 2.230531  [41600/52736]\n",
      "Train accuracy: 31.6%, loss: 2.261852  [44800/52736]\n",
      "Train accuracy: 30.7%, loss: 2.261855  [48000/52736]\n",
      "Train accuracy: 30.5%, loss: 2.261818  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240402 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train accuracy: 0.4%, loss: 2.168102  [    0/52736]\n",
      "Train accuracy: 31.7%, loss: 2.261784  [ 3200/52736]\n",
      "Train accuracy: 31.4%, loss: 2.199295  [ 6400/52736]\n",
      "Train accuracy: 31.1%, loss: 2.199558  [ 9600/52736]\n",
      "Train accuracy: 31.4%, loss: 2.355549  [12800/52736]\n",
      "Train accuracy: 31.2%, loss: 2.168245  [16000/52736]\n",
      "Train accuracy: 31.8%, loss: 2.293038  [19200/52736]\n",
      "Train accuracy: 30.4%, loss: 2.324294  [22400/52736]\n",
      "Train accuracy: 30.3%, loss: 2.105553  [25600/52736]\n",
      "Train accuracy: 30.3%, loss: 2.355578  [28800/52736]\n",
      "Train accuracy: 30.8%, loss: 2.261793  [32000/52736]\n",
      "Train accuracy: 31.1%, loss: 2.136789  [35200/52736]\n",
      "Train accuracy: 31.1%, loss: 2.230556  [38400/52736]\n",
      "Train accuracy: 30.4%, loss: 2.418021  [41600/52736]\n",
      "Train accuracy: 30.8%, loss: 2.230520  [44800/52736]\n",
      "Train accuracy: 30.6%, loss: 2.136760  [48000/52736]\n",
      "Train accuracy: 30.4%, loss: 2.261788  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train accuracy: 0.3%, loss: 2.230543  [    0/52736]\n",
      "Train accuracy: 30.8%, loss: 2.199315  [ 3200/52736]\n",
      "Train accuracy: 30.9%, loss: 2.324287  [ 6400/52736]\n",
      "Train accuracy: 31.2%, loss: 2.261812  [ 9600/52736]\n",
      "Train accuracy: 30.8%, loss: 2.168048  [12800/52736]\n",
      "Train accuracy: 30.8%, loss: 2.168055  [16000/52736]\n",
      "Train accuracy: 31.3%, loss: 2.324282  [19200/52736]\n",
      "Train accuracy: 31.3%, loss: 2.293035  [22400/52736]\n",
      "Train accuracy: 29.0%, loss: 2.168041  [25600/52736]\n",
      "Train accuracy: 29.2%, loss: 2.324290  [28800/52736]\n",
      "Train accuracy: 30.5%, loss: 2.261799  [32000/52736]\n",
      "Train accuracy: 31.2%, loss: 2.324290  [35200/52736]\n",
      "Train accuracy: 31.6%, loss: 2.105535  [38400/52736]\n",
      "Train accuracy: 30.3%, loss: 2.355541  [41600/52736]\n",
      "Train accuracy: 32.5%, loss: 2.199285  [44800/52736]\n",
      "Train accuracy: 31.6%, loss: 2.136892  [48000/52736]\n",
      "Train accuracy: 31.9%, loss: 2.293037  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train accuracy: 0.3%, loss: 2.230482  [    0/52736]\n",
      "Train accuracy: 29.1%, loss: 2.386794  [ 3200/52736]\n",
      "Train accuracy: 32.0%, loss: 2.105546  [ 6400/52736]\n",
      "Train accuracy: 30.9%, loss: 2.293035  [ 9600/52736]\n",
      "Train accuracy: 31.2%, loss: 2.230543  [12800/52736]\n",
      "Train accuracy: 30.4%, loss: 2.199286  [16000/52736]\n",
      "Train accuracy: 30.3%, loss: 2.293039  [19200/52736]\n",
      "Train accuracy: 31.2%, loss: 2.261808  [22400/52736]\n",
      "Train accuracy: 31.2%, loss: 2.136793  [25600/52736]\n",
      "Train accuracy: 30.9%, loss: 2.324272  [28800/52736]\n",
      "Train accuracy: 31.0%, loss: 2.230539  [32000/52736]\n",
      "Train accuracy: 31.3%, loss: 2.199287  [35200/52736]\n",
      "Train accuracy: 30.0%, loss: 2.168042  [38400/52736]\n",
      "Train accuracy: 30.2%, loss: 2.261734  [41600/52736]\n",
      "Train accuracy: 31.6%, loss: 2.168040  [44800/52736]\n",
      "Train accuracy: 30.6%, loss: 2.168038  [48000/52736]\n",
      "Train accuracy: 32.2%, loss: 2.261794  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train accuracy: 0.2%, loss: 2.324304  [    0/52736]\n",
      "Train accuracy: 30.2%, loss: 2.230551  [ 3200/52736]\n",
      "Train accuracy: 31.0%, loss: 2.355538  [ 6400/52736]\n",
      "Train accuracy: 31.0%, loss: 2.324291  [ 9600/52736]\n",
      "Train accuracy: 30.8%, loss: 2.074291  [12800/52736]\n",
      "Train accuracy: 30.0%, loss: 2.261792  [16000/52736]\n",
      "Train accuracy: 31.5%, loss: 2.261765  [19200/52736]\n",
      "Train accuracy: 31.0%, loss: 2.261777  [22400/52736]\n",
      "Train accuracy: 30.3%, loss: 2.324292  [25600/52736]\n",
      "Train accuracy: 31.5%, loss: 2.324270  [28800/52736]\n",
      "Train accuracy: 29.1%, loss: 2.293040  [32000/52736]\n",
      "Train accuracy: 30.6%, loss: 2.293066  [35200/52736]\n",
      "Train accuracy: 32.0%, loss: 2.168029  [38400/52736]\n",
      "Train accuracy: 31.8%, loss: 2.230539  [41600/52736]\n",
      "Train accuracy: 31.8%, loss: 2.355540  [44800/52736]\n",
      "Train accuracy: 31.5%, loss: 2.261786  [48000/52736]\n",
      "Train accuracy: 31.1%, loss: 2.136879  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train accuracy: 0.3%, loss: 2.199287  [    0/52736]\n",
      "Train accuracy: 31.3%, loss: 2.230552  [ 3200/52736]\n",
      "Train accuracy: 30.3%, loss: 2.168051  [ 6400/52736]\n",
      "Train accuracy: 30.3%, loss: 2.261807  [ 9600/52736]\n",
      "Train accuracy: 30.7%, loss: 2.199270  [12800/52736]\n",
      "Train accuracy: 30.2%, loss: 2.324308  [16000/52736]\n",
      "Train accuracy: 30.7%, loss: 2.230536  [19200/52736]\n",
      "Train accuracy: 30.3%, loss: 2.261792  [22400/52736]\n",
      "Train accuracy: 32.1%, loss: 2.418040  [25600/52736]\n",
      "Train accuracy: 32.1%, loss: 2.293040  [28800/52736]\n",
      "Train accuracy: 31.5%, loss: 2.230541  [32000/52736]\n",
      "Train accuracy: 30.2%, loss: 2.418043  [35200/52736]\n",
      "Train accuracy: 29.7%, loss: 2.355545  [38400/52736]\n",
      "Train accuracy: 29.9%, loss: 2.199287  [41600/52736]\n",
      "Train accuracy: 32.4%, loss: 2.199324  [44800/52736]\n",
      "Train accuracy: 32.1%, loss: 2.168039  [48000/52736]\n",
      "Train accuracy: 31.5%, loss: 2.199321  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train accuracy: 0.3%, loss: 2.199286  [    0/52736]\n",
      "Train accuracy: 31.7%, loss: 2.105547  [ 3200/52736]\n",
      "Train accuracy: 30.2%, loss: 2.230545  [ 6400/52736]\n",
      "Train accuracy: 31.1%, loss: 2.261795  [ 9600/52736]\n",
      "Train accuracy: 31.2%, loss: 2.293040  [12800/52736]\n",
      "Train accuracy: 30.8%, loss: 2.167987  [16000/52736]\n",
      "Train accuracy: 30.5%, loss: 2.355376  [19200/52736]\n",
      "Train accuracy: 30.2%, loss: 2.230546  [22400/52736]\n",
      "Train accuracy: 29.9%, loss: 2.199291  [25600/52736]\n",
      "Train accuracy: 31.2%, loss: 2.324288  [28800/52736]\n",
      "Train accuracy: 29.9%, loss: 2.199290  [32000/52736]\n",
      "Train accuracy: 30.0%, loss: 2.168048  [35200/52736]\n",
      "Train accuracy: 31.5%, loss: 2.199295  [38400/52736]\n",
      "Train accuracy: 32.8%, loss: 2.199286  [41600/52736]\n",
      "Train accuracy: 30.8%, loss: 2.199289  [44800/52736]\n",
      "Train accuracy: 31.4%, loss: 2.355538  [48000/52736]\n",
      "Train accuracy: 31.2%, loss: 2.355541  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train accuracy: 0.5%, loss: 2.043036  [    0/52736]\n",
      "Train accuracy: 30.9%, loss: 2.136791  [ 3200/52736]\n",
      "Train accuracy: 30.3%, loss: 2.136795  [ 6400/52736]\n",
      "Train accuracy: 32.2%, loss: 2.074283  [ 9600/52736]\n",
      "Train accuracy: 30.3%, loss: 2.324285  [12800/52736]\n",
      "Train accuracy: 31.4%, loss: 2.105542  [16000/52736]\n",
      "Train accuracy: 30.2%, loss: 2.199298  [19200/52736]\n",
      "Train accuracy: 31.2%, loss: 2.168039  [22400/52736]\n",
      "Train accuracy: 32.8%, loss: 2.199290  [25600/52736]\n",
      "Train accuracy: 31.2%, loss: 2.199290  [28800/52736]\n",
      "Train accuracy: 31.2%, loss: 2.386790  [32000/52736]\n",
      "Train accuracy: 30.4%, loss: 2.199291  [35200/52736]\n",
      "Train accuracy: 30.5%, loss: 2.199291  [38400/52736]\n",
      "Train accuracy: 30.7%, loss: 2.230546  [41600/52736]\n",
      "Train accuracy: 30.9%, loss: 2.199290  [44800/52736]\n",
      "Train accuracy: 30.0%, loss: 2.293037  [48000/52736]\n",
      "Train accuracy: 30.0%, loss: 2.230541  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train accuracy: 0.2%, loss: 2.293040  [    0/52736]\n",
      "Train accuracy: 31.8%, loss: 2.293041  [ 3200/52736]\n",
      "Train accuracy: 30.8%, loss: 2.168052  [ 6400/52736]\n",
      "Train accuracy: 31.2%, loss: 2.168056  [ 9600/52736]\n",
      "Train accuracy: 30.6%, loss: 2.324290  [12800/52736]\n",
      "Train accuracy: 32.6%, loss: 2.230523  [16000/52736]\n",
      "Train accuracy: 31.4%, loss: 2.355536  [19200/52736]\n",
      "Train accuracy: 31.4%, loss: 2.199293  [22400/52736]\n",
      "Train accuracy: 29.9%, loss: 2.293040  [25600/52736]\n",
      "Train accuracy: 30.3%, loss: 2.324290  [28800/52736]\n",
      "Train accuracy: 30.5%, loss: 2.230536  [32000/52736]\n",
      "Train accuracy: 32.2%, loss: 2.293058  [35200/52736]\n",
      "Train accuracy: 30.5%, loss: 2.168044  [38400/52736]\n",
      "Train accuracy: 31.1%, loss: 2.230541  [41600/52736]\n",
      "Train accuracy: 31.6%, loss: 2.386784  [44800/52736]\n",
      "Train accuracy: 29.5%, loss: 2.230561  [48000/52736]\n",
      "Train accuracy: 29.8%, loss: 2.136792  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train accuracy: 0.2%, loss: 2.324290  [    0/52736]\n",
      "Train accuracy: 31.6%, loss: 2.261790  [ 3200/52736]\n",
      "Train accuracy: 30.0%, loss: 2.105541  [ 6400/52736]\n",
      "Train accuracy: 31.8%, loss: 2.230519  [ 9600/52736]\n",
      "Train accuracy: 30.7%, loss: 2.355535  [12800/52736]\n",
      "Train accuracy: 31.5%, loss: 2.324288  [16000/52736]\n",
      "Train accuracy: 29.4%, loss: 2.136791  [19200/52736]\n",
      "Train accuracy: 31.5%, loss: 2.168040  [22400/52736]\n",
      "Train accuracy: 31.0%, loss: 2.261790  [25600/52736]\n",
      "Train accuracy: 31.8%, loss: 2.136791  [28800/52736]\n",
      "Train accuracy: 30.3%, loss: 2.324289  [32000/52736]\n",
      "Train accuracy: 30.6%, loss: 2.230540  [35200/52736]\n",
      "Train accuracy: 31.5%, loss: 2.199297  [38400/52736]\n",
      "Train accuracy: 31.0%, loss: 2.293040  [41600/52736]\n",
      "Train accuracy: 31.4%, loss: 2.261802  [44800/52736]\n",
      "Train accuracy: 30.6%, loss: 2.199291  [48000/52736]\n",
      "Train accuracy: 30.4%, loss: 2.199296  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train accuracy: 0.3%, loss: 2.261798  [    0/52736]\n",
      "Train accuracy: 31.3%, loss: 2.261782  [ 3200/52736]\n",
      "Train accuracy: 30.1%, loss: 2.261789  [ 6400/52736]\n",
      "Train accuracy: 30.6%, loss: 2.449289  [ 9600/52736]\n",
      "Train accuracy: 30.5%, loss: 2.293038  [12800/52736]\n",
      "Train accuracy: 31.3%, loss: 2.261790  [16000/52736]\n",
      "Train accuracy: 31.5%, loss: 2.293041  [19200/52736]\n",
      "Train accuracy: 31.2%, loss: 2.293040  [22400/52736]\n",
      "Train accuracy: 33.1%, loss: 2.230545  [25600/52736]\n",
      "Train accuracy: 31.0%, loss: 2.199281  [28800/52736]\n",
      "Train accuracy: 30.0%, loss: 2.168060  [32000/52736]\n",
      "Train accuracy: 30.3%, loss: 2.199285  [35200/52736]\n",
      "Train accuracy: 30.4%, loss: 2.168042  [38400/52736]\n",
      "Train accuracy: 30.8%, loss: 2.199286  [41600/52736]\n",
      "Train accuracy: 30.9%, loss: 2.230539  [44800/52736]\n",
      "Train accuracy: 31.6%, loss: 2.261790  [48000/52736]\n",
      "Train accuracy: 30.4%, loss: 2.074293  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Train accuracy: 0.3%, loss: 2.199292  [    0/52736]\n",
      "Train accuracy: 30.7%, loss: 2.105541  [ 3200/52736]\n",
      "Train accuracy: 32.3%, loss: 2.105541  [ 6400/52736]\n",
      "Train accuracy: 30.5%, loss: 2.261793  [ 9600/52736]\n",
      "Train accuracy: 31.2%, loss: 2.136790  [12800/52736]\n",
      "Train accuracy: 31.1%, loss: 2.136796  [16000/52736]\n",
      "Train accuracy: 31.2%, loss: 2.168047  [19200/52736]\n",
      "Train accuracy: 30.6%, loss: 2.261790  [22400/52736]\n",
      "Train accuracy: 30.8%, loss: 2.261791  [25600/52736]\n",
      "Train accuracy: 30.8%, loss: 2.418040  [28800/52736]\n",
      "Train accuracy: 30.2%, loss: 2.199290  [32000/52736]\n",
      "Train accuracy: 30.8%, loss: 2.261786  [35200/52736]\n",
      "Train accuracy: 30.5%, loss: 2.261791  [38400/52736]\n",
      "Train accuracy: 31.8%, loss: 2.230540  [41600/52736]\n",
      "Train accuracy: 31.1%, loss: 2.261791  [44800/52736]\n",
      "Train accuracy: 30.6%, loss: 2.261795  [48000/52736]\n",
      "Train accuracy: 29.8%, loss: 2.136792  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Train accuracy: 0.3%, loss: 2.230546  [    0/52736]\n",
      "Train accuracy: 31.9%, loss: 2.355534  [ 3200/52736]\n",
      "Train accuracy: 32.9%, loss: 2.168041  [ 6400/52736]\n",
      "Train accuracy: 29.3%, loss: 2.230541  [ 9600/52736]\n",
      "Train accuracy: 31.6%, loss: 2.261785  [12800/52736]\n",
      "Train accuracy: 30.8%, loss: 2.230540  [16000/52736]\n",
      "Train accuracy: 30.4%, loss: 2.074290  [19200/52736]\n",
      "Train accuracy: 31.1%, loss: 2.199297  [22400/52736]\n",
      "Train accuracy: 30.6%, loss: 2.136830  [25600/52736]\n",
      "Train accuracy: 31.4%, loss: 2.355540  [28800/52736]\n",
      "Train accuracy: 30.5%, loss: 2.293040  [32000/52736]\n",
      "Train accuracy: 31.0%, loss: 2.199291  [35200/52736]\n",
      "Train accuracy: 30.8%, loss: 2.324290  [38400/52736]\n",
      "Train accuracy: 30.5%, loss: 2.324301  [41600/52736]\n",
      "Train accuracy: 30.8%, loss: 2.199296  [44800/52736]\n",
      "Train accuracy: 29.3%, loss: 2.293039  [48000/52736]\n",
      "Train accuracy: 32.3%, loss: 2.168039  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Train accuracy: 0.3%, loss: 2.261791  [    0/52736]\n",
      "Train accuracy: 31.3%, loss: 2.168043  [ 3200/52736]\n",
      "Train accuracy: 31.2%, loss: 2.324289  [ 6400/52736]\n",
      "Train accuracy: 30.6%, loss: 2.261788  [ 9600/52736]\n",
      "Train accuracy: 30.9%, loss: 2.324290  [12800/52736]\n",
      "Train accuracy: 30.6%, loss: 2.199289  [16000/52736]\n",
      "Train accuracy: 32.3%, loss: 2.293035  [19200/52736]\n",
      "Train accuracy: 31.0%, loss: 2.261787  [22400/52736]\n",
      "Train accuracy: 30.5%, loss: 2.261790  [25600/52736]\n",
      "Train accuracy: 29.5%, loss: 2.136791  [28800/52736]\n",
      "Train accuracy: 31.3%, loss: 2.261791  [32000/52736]\n",
      "Train accuracy: 30.1%, loss: 2.199298  [35200/52736]\n",
      "Train accuracy: 32.8%, loss: 2.261791  [38400/52736]\n",
      "Train accuracy: 31.0%, loss: 2.261790  [41600/52736]\n",
      "Train accuracy: 30.3%, loss: 2.261790  [44800/52736]\n",
      "Train accuracy: 31.4%, loss: 2.261791  [48000/52736]\n",
      "Train accuracy: 29.6%, loss: 2.136791  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Train accuracy: 0.3%, loss: 2.261787  [    0/52736]\n",
      "Train accuracy: 31.8%, loss: 2.355540  [ 3200/52736]\n",
      "Train accuracy: 29.5%, loss: 2.261788  [ 6400/52736]\n",
      "Train accuracy: 30.4%, loss: 2.043036  [ 9600/52736]\n",
      "Train accuracy: 31.9%, loss: 2.199289  [12800/52736]\n",
      "Train accuracy: 29.8%, loss: 2.168005  [16000/52736]\n",
      "Train accuracy: 30.2%, loss: 2.261796  [19200/52736]\n",
      "Train accuracy: 30.9%, loss: 2.261789  [22400/52736]\n",
      "Train accuracy: 31.7%, loss: 2.480540  [25600/52736]\n",
      "Train accuracy: 31.9%, loss: 2.230550  [28800/52736]\n",
      "Train accuracy: 31.4%, loss: 2.168043  [32000/52736]\n",
      "Train accuracy: 31.2%, loss: 2.136790  [35200/52736]\n",
      "Train accuracy: 30.6%, loss: 2.199290  [38400/52736]\n",
      "Train accuracy: 31.2%, loss: 2.230540  [41600/52736]\n",
      "Train accuracy: 30.4%, loss: 2.074290  [44800/52736]\n",
      "Train accuracy: 31.5%, loss: 2.168040  [48000/52736]\n",
      "Train accuracy: 30.2%, loss: 2.199290  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Train accuracy: 0.3%, loss: 2.230539  [    0/52736]\n",
      "Train accuracy: 30.2%, loss: 2.230540  [ 3200/52736]\n",
      "Train accuracy: 30.0%, loss: 2.293041  [ 6400/52736]\n",
      "Train accuracy: 30.8%, loss: 2.355521  [ 9600/52736]\n",
      "Train accuracy: 30.0%, loss: 2.230540  [12800/52736]\n",
      "Train accuracy: 31.8%, loss: 2.105540  [16000/52736]\n",
      "Train accuracy: 32.1%, loss: 2.199290  [19200/52736]\n",
      "Train accuracy: 31.8%, loss: 2.199315  [22400/52736]\n",
      "Train accuracy: 32.1%, loss: 2.199290  [25600/52736]\n",
      "Train accuracy: 30.1%, loss: 2.355535  [28800/52736]\n",
      "Train accuracy: 30.9%, loss: 2.074289  [32000/52736]\n",
      "Train accuracy: 30.5%, loss: 2.230540  [35200/52736]\n",
      "Train accuracy: 31.0%, loss: 2.261790  [38400/52736]\n",
      "Train accuracy: 31.8%, loss: 2.386790  [41600/52736]\n",
      "Train accuracy: 30.4%, loss: 2.136789  [44800/52736]\n",
      "Train accuracy: 30.9%, loss: 2.136791  [48000/52736]\n",
      "Train accuracy: 31.4%, loss: 2.105538  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Train accuracy: 0.5%, loss: 2.074291  [    0/52736]\n",
      "Train accuracy: 30.6%, loss: 2.230541  [ 3200/52736]\n",
      "Train accuracy: 30.9%, loss: 2.261790  [ 6400/52736]\n",
      "Train accuracy: 30.4%, loss: 2.136793  [ 9600/52736]\n",
      "Train accuracy: 30.6%, loss: 2.168041  [12800/52736]\n",
      "Train accuracy: 29.8%, loss: 2.293041  [16000/52736]\n",
      "Train accuracy: 30.4%, loss: 2.168041  [19200/52736]\n",
      "Train accuracy: 30.8%, loss: 2.199291  [22400/52736]\n",
      "Train accuracy: 32.2%, loss: 2.261790  [25600/52736]\n",
      "Train accuracy: 31.9%, loss: 2.230537  [28800/52736]\n",
      "Train accuracy: 31.4%, loss: 2.324290  [32000/52736]\n",
      "Train accuracy: 30.7%, loss: 2.418040  [35200/52736]\n",
      "Train accuracy: 31.9%, loss: 2.074290  [38400/52736]\n",
      "Train accuracy: 31.3%, loss: 2.230540  [41600/52736]\n",
      "Train accuracy: 30.3%, loss: 2.293040  [44800/52736]\n",
      "Train accuracy: 30.4%, loss: 2.386791  [48000/52736]\n",
      "Train accuracy: 31.6%, loss: 2.293040  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Train accuracy: 0.2%, loss: 2.293040  [    0/52736]\n",
      "Train accuracy: 32.1%, loss: 2.324290  [ 3200/52736]\n",
      "Train accuracy: 29.9%, loss: 2.230531  [ 6400/52736]\n",
      "Train accuracy: 32.1%, loss: 2.261793  [ 9600/52736]\n",
      "Train accuracy: 30.6%, loss: 2.324295  [12800/52736]\n",
      "Train accuracy: 31.7%, loss: 2.168049  [16000/52736]\n",
      "Train accuracy: 31.2%, loss: 2.418040  [19200/52736]\n",
      "Train accuracy: 33.0%, loss: 2.261791  [22400/52736]\n",
      "Train accuracy: 31.1%, loss: 2.261791  [25600/52736]\n",
      "Train accuracy: 30.1%, loss: 2.074290  [28800/52736]\n",
      "Train accuracy: 29.3%, loss: 2.293040  [32000/52736]\n",
      "Train accuracy: 30.0%, loss: 2.355540  [35200/52736]\n",
      "Train accuracy: 31.2%, loss: 2.230540  [38400/52736]\n",
      "Train accuracy: 31.0%, loss: 2.355540  [41600/52736]\n",
      "Train accuracy: 29.6%, loss: 2.199290  [44800/52736]\n",
      "Train accuracy: 29.8%, loss: 2.261791  [48000/52736]\n",
      "Train accuracy: 30.4%, loss: 2.168051  [51200/52736]\n",
      "3990.0\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.240401 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Train accuracy: 0.2%, loss: 2.355541  [    0/52736]\n",
      "Train accuracy: 29.5%, loss: 2.324292  [ 3200/52736]\n",
      "Train accuracy: 30.8%, loss: 2.293040  [ 6400/52736]\n",
      "Train accuracy: 30.8%, loss: 2.261790  [ 9600/52736]\n",
      "Train accuracy: 30.9%, loss: 2.136791  [12800/52736]\n",
      "Train accuracy: 30.2%, loss: 2.324291  [16000/52736]\n",
      "Train accuracy: 30.0%, loss: 2.293041  [19200/52736]\n",
      "Train accuracy: 31.9%, loss: 2.324291  [22400/52736]\n",
      "Train accuracy: 31.0%, loss: 2.230540  [25600/52736]\n",
      "Train accuracy: 31.0%, loss: 2.199291  [28800/52736]\n",
      "Train accuracy: 31.8%, loss: 2.230542  [32000/52736]\n",
      "Train accuracy: 29.9%, loss: 2.199292  [35200/52736]\n",
      "Train accuracy: 31.1%, loss: 2.199290  [38400/52736]\n",
      "Train accuracy: 31.9%, loss: 2.199291  [41600/52736]\n",
      "Train accuracy: 30.6%, loss: 2.136787  [44800/52736]\n",
      "Train accuracy: 31.4%, loss: 1.980543  [48000/52736]\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, combined_model, loss_fn, optimizer)\n",
    "    test(validation_loader, combined_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aca73733-9eb1-4eb4-9e0c-f2a2a0ce1f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in combined_model.named_parameters():\n",
    "    print(name, param.max(), param.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc7c20-ea1c-4a8d-8a63-e3510f6613e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in combined_model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "21dd6976-2636-4526-87fb-366586cd2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for standard gcn\n",
    "\n",
    "trained_model_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f3dc5b4-884e-476d-b6d7-97ca333f945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skeleton for loading trained encoder\n",
    "\n",
    "class trained_encoder_att(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, nhidden, p, adj, batch_size, dropout):\n",
    "        super(trained_encoder_att, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.nhidden = nhidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.poolsize = 8\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.p = p\n",
    "        self.adj = Parameter(torch.from_numpy(adj).contiguous().float(), requires_grad=False).to(self.device)\n",
    "        self.gcn = GraphAttConvolution(self.input_dim, 1, self.nhidden, self.batch_size, bias=False)\n",
    "#         self.gcn2 = GraphConvolution(self.input_dim, self.nhidden, self.nhidden//2, self.batch_size, bias=False)\n",
    "        self.encoder_a1 = nn.Linear(int(((self.input_dim//self.p)*nhidden)), 128, bias=True)\n",
    "        self.encoder_a2 = nn.Linear(128, 32, bias=True)\n",
    "        \n",
    "        #Make sure gene order in adj is same!\n",
    "        #how to implement the attention layer?\n",
    "        #what's the diff between Parameter and Variable\n",
    "        \n",
    "    def graph_max_pool(self, x, p):\n",
    "        if p > 1:\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x F x V\n",
    "            x = nn.MaxPool1d(p)(x)             # B x F x V/p\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x V/p x F\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #encoder\n",
    "        stream1 = torch.unsqueeze(x, -1)\n",
    "        stream1 = F.relu(self.gcn(stream1, self.adj))\n",
    "        stream1 = self.graph_max_pool(stream1, self.p)\n",
    "        stream1 = torch.flatten(stream1, start_dim = 1)\n",
    "        stream1 = F.relu(self.encoder_a1(stream1))\n",
    "        outputs = F.relu(self.encoder_a2(stream1))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a8928346-14a8-4981-81d2-c44c56745dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skeleton for loading trained encoder\n",
    "\n",
    "class trained_encoder_gcn(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, nhidden, p, adj, batch_size, dropout):\n",
    "        super(trained_encoder_gcn, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.nhidden = nhidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.poolsize = 8\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.p = p\n",
    "        self.adj = Parameter(torch.from_numpy(adj).contiguous().float(), requires_grad=False).to(self.device)\n",
    "        self.gcn = GraphConvolution(self.input_dim, 1, self.nhidden, self.batch_size, bias=False)\n",
    "#         self.gcn2 = GraphConvolution(self.input_dim, self.nhidden, self.nhidden//2, self.batch_size, bias=False)\n",
    "        self.encoder_a1 = nn.Linear(int(((self.input_dim//self.p)*nhidden)), 128, bias=True)\n",
    "        self.encoder_a2 = nn.Linear(128, 32, bias=True)\n",
    "        \n",
    "        #Make sure gene order in adj is same!\n",
    "        #how to implement the attention layer?\n",
    "        #what's the diff between Parameter and Variable\n",
    "        \n",
    "    def graph_max_pool(self, x, p):\n",
    "        if p > 1:\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x F x V\n",
    "            x = nn.MaxPool1d(p)(x)             # B x F x V/p\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x V/p x F\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #encoder\n",
    "        stream1 = torch.unsqueeze(x, -1)\n",
    "        stream1 = F.relu(self.gcn(stream1, self.adj))\n",
    "        stream1 = self.graph_max_pool(stream1, self.p)\n",
    "        stream1 = torch.flatten(stream1, start_dim = 1)\n",
    "        stream1 = F.relu(self.encoder_a1(stream1))\n",
    "        outputs = F.relu(self.encoder_a2(stream1))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2814234a-3520-46ad-8491-e8166e3f6120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#Trained encoder\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "trained_encoder_model = trained_encoder_gcn(pbmc.shape[1], nhidden=32, p=8, adj=L, batch_size=batch_size, dropout=0.3)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "84caec80-289f-4c50-8b7b-295d0559b7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_encoder_model_dict = trained_encoder_model.state_dict()\n",
    "\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in trained_model_dict.items() if k in trained_encoder_model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "trained_encoder_model_dict.update(pretrained_dict) \n",
    "# # 3. load the new state dict\n",
    "trained_encoder_model.load_state_dict(pretrained_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "edd82c7f-0d49-4a43-a77b-ff4c60291b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_input.weight\n",
      "fc_input.bias\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "fc2.weight\n",
      "fc2.bias\n",
      "fc3.weight\n",
      "fc3.bias\n"
     ]
    }
   ],
   "source": [
    "for k,v in trained_model_dict.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "3c00cdac-cf60-460d-a146-9f9eff4a79c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj\n",
      "gcn.weight\n",
      "encoder_a1.weight\n",
      "encoder_a1.bias\n",
      "encoder_a2.weight\n",
      "encoder_a2.bias\n"
     ]
    }
   ],
   "source": [
    "for k,v in trained_encoder_model_dict.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c09dd4d1-55ab-4d60-ad29-3c7703421358",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in trained_encoder_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93646fd-d711-4f0e-ae1c-0a220692a0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

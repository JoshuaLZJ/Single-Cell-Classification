{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1098257b-4f08-44d2-b6ff-c357dad0e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notebook for training a GCN network to classify cells without the autoencoder training first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f38651b3-34e8-4b89-ba74-a1a66e198393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch_geometric.utils import get_laplacian\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import networkx as nx\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3070959f-e73c-4343-9327-3e6b50c62b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680bb518-cb4b-4ba5-a5a1-e268a7433602",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('data/Labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0110e5b-9802-40a3-a1b7-03f83a808c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CD8+ Cytotoxic T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CD8+/CD45RA+ Naive Cytotoxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CD4+/CD45RO+ Memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CD19+ B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CD4+/CD25 T Reg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>CD8+/CD45RA+ Naive Cytotoxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>CD8+/CD45RA+ Naive Cytotoxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>CD4+/CD45RA+/CD25- Naive T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>CD4+/CD45RA+/CD25- Naive T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>CD4+/CD25 T Reg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               x\n",
       "0               CD8+ Cytotoxic T\n",
       "1   CD8+/CD45RA+ Naive Cytotoxic\n",
       "2            CD4+/CD45RO+ Memory\n",
       "3                        CD19+ B\n",
       "4                CD4+/CD25 T Reg\n",
       "..                           ...\n",
       "95  CD8+/CD45RA+ Naive Cytotoxic\n",
       "96  CD8+/CD45RA+ Naive Cytotoxic\n",
       "97    CD4+/CD45RA+/CD25- Naive T\n",
       "98    CD4+/CD45RA+/CD25- Naive T\n",
       "99               CD4+/CD25 T Reg\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c67a01-c95d-433d-b3b3-df211cb7b3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0    1    2    3    4    5    6    7    8    9    10\n",
      "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
      "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
      "2      0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
      "3      0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "4      0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
      "65938  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
      "65939  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
      "65940  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
      "65941  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
      "65942  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
      "\n",
      "[65943 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "labels_df = pd.DataFrame(encoder.fit_transform(labels[['x']]).toarray())\n",
    "\n",
    "print(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab17511-4757-4202-9c5f-d6c06ecb88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbmc = np.load('data/pbmc_normalized_filtered2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee6b4cb5-d1d0-4ec1-bafb-f3114030a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing classification using a simple FF network.\n",
    "\n",
    "class simpleNet(nn.Module):\n",
    "    def __init__(self,input_dim,device,coef_m=None):\n",
    "        super(simpleNet, self).__init__()\n",
    "        self.fc_input=nn.Linear(input_dim,1024)\n",
    "        self.fc1=nn.Linear(1024,512)\n",
    "        self.fc2=nn.Linear(512,128)\n",
    "        self.fc3=nn.Linear(128,10)\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "        self.device=device\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x.to(self.device)\n",
    "        x=F.relu(self.fc_input(x))\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        outputs=torch.sigmoid(self.fc3(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd66c5db-82b5-48b0-843e-d00e567d836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "simpleNet(\n",
      "  (fc_input): Linear(in_features=2000, out_features=1024, bias=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = simpleNet(input_dim=pbmc.shape[1], device=device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e50fbd7-9851-4714-97b0-9eceff7adb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "x_tensor = torch.tensor(pbmc.astype(np.float32))\n",
    "y_tensor = torch.tensor(labels_df.values.astype(np.float32))\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(x_tensor)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7889753b-0a6f-47b8-ae34-e97d9e2f970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = math.floor(len(dataloader.dataset)*0.8)\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            correct /= 100*batch_size\n",
    "\n",
    "            print(f\"Train accuracy: {(100*correct):>0.1f}%, loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            correct = 0\n",
    "            \n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = math.floor(len(dataloader.dataset)*0.2)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    print(correct)\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f2df4-6cba-4123-8003-fe3b8723ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(validation_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fab526d-6970-4d0d-bd82-daadf317f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading adjacency matrix and filtered gene expression\n",
    "#The following approach will be based on Graph Convolutional Network methods\n",
    "\n",
    "adj_mat = np.load('data/adj_mat_on_normalized2.npy')\n",
    "pbmc = np.load('data/pbmc_normalized_filtered2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2d638f07-95d2-4a2b-9207-434a02818751",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = adj_mat.shape[0] # number of nodes in a graph\n",
    "D = np.sum(adj_mat, 0) # node degrees\n",
    "D_hat_pos = np.diag((D + 1e-5)**(0.5)) # normalized node degrees\n",
    "D_hat_neg = np.diag((D + 1e-5)**(-0.5)) # normalized node degrees\n",
    "L = np.identity(N) + D_hat_neg.dot(adj_mat).dot(D_hat_pos) # Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "462e2509-1f85-4493-91c0-ddcb6fd055e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral convolution on graphs\n",
    "# X is an NÃ—1 matrix of 1-dimensional node features\n",
    "# L is an NÃ—N graph Laplacian computed above\n",
    "# W_spectral are NÃ—F weights (filters) that we want to train\n",
    "from scipy.sparse.linalg import eigsh # assumes L to be symmetric\n",
    "\n",
    "Î›,V = eigsh(L,k=20,which='SM') # eigen-decomposition (i.e. find Î›,V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "68829790-ba47-416f-a917-580acdec93ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph convolution model 1\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, in_features, out_features, batch_size, bias=False):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(self.in_features, self.out_features), requires_grad=True)\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(self.input_dim, self.out_features), requires_grad=True)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "#         stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "#         self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.xavier_uniform_(self.bias, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "#         input = torch.permute(input, (0, 2, 1))\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.matmul(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "    \n",
    "class full_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, nhidden, p, output_dim, adj, batch_size, dropout):\n",
    "        super(full_net, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.nhidden = nhidden\n",
    "        self.output_dim = output_dim\n",
    "#         self.fc1 = nn.Linear(input_dim, nhidden, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.poolsize = 8\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.adj = Parameter(torch.from_numpy(adj).contiguous().float(), requires_grad=False).to(self.device)\n",
    "        self.p = p\n",
    "        self.gcn = GraphConvolution(self.input_dim, 1, self.nhidden, self.batch_size, bias=False)\n",
    "#         self.gcn2 = GraphConvolution(self.input_dim, self.nhidden, self.nhidden//2, self.batch_size, bias=False)\n",
    "        self.fc1 = nn.Linear(int(((self.input_dim//self.p)*nhidden)), 128, bias=True)\n",
    "        self.fc2 = nn.Linear(128, 32, bias=True)\n",
    "        self.fc3 = nn.Linear(self.input_dim, 128, bias=True)\n",
    "        self.fc4 = nn.Linear(128, 32, bias=True)\n",
    "        self.fc5 = nn.Linear(32, self.output_dim, bias=True)\n",
    "        \n",
    "        #Make sure gene order in adj is same!\n",
    "        #how to implement the attention layer?\n",
    "        #what's the diff between Parameter and Variable\n",
    "        \n",
    "    def graph_max_pool(self, x, p):\n",
    "        if p > 1:\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x F x V\n",
    "            x = nn.MaxPool1d(p)(x)             # B x F x V/p\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x V/p x F\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        stream1 = torch.unsqueeze(x, -1)\n",
    "        stream1 = F.relu(self.gcn(stream1, self.adj))\n",
    "#         stream1 = F.relu(self.gcn2(stream1, self.adj))\n",
    "        stream1 = self.graph_max_pool(stream1, self.p)\n",
    "        stream1 = torch.flatten(stream1, start_dim = 1)\n",
    "        stream1 = F.relu(self.fc1(stream1))\n",
    "        stream1 = self.dropout(stream1)\n",
    "        stream1 = F.relu(self.fc2(stream1))\n",
    "        \n",
    "#         stream2 = F.relu(self.fc3(x))\n",
    "#         stream2 = self.dropout(stream2)\n",
    "#         stream2 = F.relu(self.fc4(stream2))\n",
    "        \n",
    "#         combined = torch.cat((stream1, stream2), -1)\n",
    "        \n",
    "        outputs = torch.sigmoid(self.fc5(stream1))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dfcd6724-ecdf-4e3e-8a8f-ecc02588e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph convolution model with att\n",
    "\n",
    "class GraphAttConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, in_features, out_features, batch_size, bias=False):\n",
    "        super(GraphAttConvolution, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.att = Parameter(torch.FloatTensor(self.input_dim, self.input_dim), requires_grad=True)\n",
    "        self.weight = Parameter(torch.FloatTensor(self.in_features, self.out_features), requires_grad=True)\n",
    "#         self.att = Parameter(torch.FloatTensor(self.input_dim, self.out_features), requires_grad=False)\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(self.input_dim, self.out_features), requires_grad=True)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        torch.nn.init.xavier_uniform_(self.att, gain=nn.init.calculate_gain('relu'))\n",
    "#         stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "#         self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.xavier_uniform_(self.bias, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "#         input = torch.permute(input, (0, 2, 1))\n",
    "        self.att = Parameter(torch.softmax(self.att, dim=1), requires_grad=True)\n",
    "        adj = torch.matmul(self.att, adj)\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.matmul(adj, support)\n",
    "#         att = torch.softmax(output, dim=1)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "    \n",
    "class full_net_att(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, nhidden, p, output_dim, adj, batch_size, dropout):\n",
    "        super(full_net_att, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.nhidden = nhidden\n",
    "        self.output_dim = output_dim\n",
    "#         self.fc1 = nn.Linear(input_dim, nhidden, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.poolsize = 8\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.adj = Parameter(torch.from_numpy(adj).contiguous().float(), requires_grad=False).to(self.device)\n",
    "        self.p = p\n",
    "        self.gcn = GraphAttConvolution(self.input_dim, 1, self.nhidden, self.batch_size, bias=False)\n",
    "#         self.gcn2 = GraphAttConvolution(self.input_dim, self.nhidden, self.nhidden//2, self.batch_size, bias=False)\n",
    "        self.fc1 = nn.Linear(int(((self.input_dim//self.p)*nhidden)), 128, bias=True)\n",
    "        self.fc2 = nn.Linear(128, 32, bias=True)\n",
    "#         self.fc3 = nn.Linear(self.input_dim, 128, bias=True)\n",
    "#         self.fc4 = nn.Linear(128, 32, bias=True)\n",
    "        self.fc5 = nn.Linear(32, self.output_dim, bias=True)\n",
    "        \n",
    "        #Make sure gene order in adj is same!\n",
    "        #how to implement the attention layer?\n",
    "        #what's the diff between Parameter and Variable\n",
    "        \n",
    "    def graph_max_pool(self, x, p):\n",
    "        if p > 1:\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x F x V\n",
    "            x = nn.MaxPool1d(p)(x)             # B x F x V/p\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x V/p x F\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        stream1 = torch.unsqueeze(x, -1)\n",
    "        stream1 = F.relu(self.gcn(stream1, self.adj))\n",
    "#         stream1 = F.relu(self.gcn2(stream1, self.adj))\n",
    "        stream1 = self.graph_max_pool(stream1, self.p)\n",
    "        stream1 = torch.flatten(stream1, start_dim = 1)\n",
    "        stream1 = F.relu(self.fc1(stream1))\n",
    "        stream1 = self.dropout(stream1)\n",
    "        stream1 = F.relu(self.fc2(stream1))\n",
    "        \n",
    "#         stream2 = F.relu(self.fc3(x))\n",
    "#         stream2 = self.dropout(stream2)\n",
    "#         stream2 = F.relu(self.fc4(stream2))\n",
    "        \n",
    "#         combined = torch.cat((stream1, stream2), -1)\n",
    "        \n",
    "        outputs = torch.sigmoid(self.fc5(stream1))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ffa09852-c16f-4490-8d6a-6f4af24f554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoencoder using spectral theory model\n",
    "\n",
    "class SpectralConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, out_features, batch_size, bias=False):\n",
    "        super(SpectralConvolution, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.out_features = out_features\n",
    "        self.weight_spectral = Parameter(torch.FloatTensor(self.input_dim, self.out_features), requires_grad=True)\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(self.input_dim, self.out_features), requires_grad=True)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight_spectral, gain=nn.init.calculate_gain('relu'))\n",
    "#         stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "#         self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.xavier_uniform_(self.bias, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, input, V):\n",
    "        X_hat = torch.matmul(V.T, input) # 20Ã—1 node features in the \"spectral\" domain\n",
    "        W_hat = torch.matmul(V.T, self.weight_spectral) # 20Ã—F filters in the \"spectral\" domain\n",
    "        output = torch.matmul(V, X_hat * W_hat)  # NÃ—F result of convolution\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.out_features) + ')'\n",
    "    \n",
    "class spectral_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, nhidden, p, output_dim, adj, batch_size, dropout):\n",
    "        super(spectral_net, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.nhidden = nhidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.poolsize = 8\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.adj = Parameter(torch.from_numpy(adj).contiguous().float(), requires_grad=False).to(self.device)\n",
    "        self.p = p\n",
    "        self.gcn = SpectralConvolution(self.input_dim, self.nhidden, self.batch_size, bias=False)\n",
    "        self.gcn2 = SpectralConvolution(self.input_dim, self.nhidden, self.batch_size, bias=False)\n",
    "        self.fc1 = nn.Linear(int(((self.input_dim//self.p)*nhidden)), 128, bias=True)\n",
    "        self.fc2 = nn.Linear(128, 32, bias=True)\n",
    "        \n",
    "        self.fc3 = nn.Linear(self.input_dim, 128, bias=True)\n",
    "        self.fc4 = nn.Linear(128, 32, bias=True)\n",
    "        self.fc5 = nn.Linear(64, self.output_dim, bias=True)\n",
    "\n",
    "        \n",
    "        #Make sure gene order in adj is same!\n",
    "        #how to implement the attention layer?\n",
    "        #what's the diff between Parameter and Variable\n",
    "        \n",
    "    def graph_max_pool(self, x, p):\n",
    "        if p > 1:\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x F x V\n",
    "            x = nn.MaxPool1d(p)(x)             # B x F x V/p\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x V/p x F\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        stream1 = torch.unsqueeze(x, -1)\n",
    "        stream1 = F.relu(self.gcn(stream1, self.adj))\n",
    "        stream1 = F.relu(self.gcn2(stream1, self.adj))\n",
    "        stream1 = self.graph_max_pool(stream1, self.p)\n",
    "        stream1 = torch.flatten(stream1, start_dim = 1)\n",
    "        stream1 = F.relu(self.fc1(stream1))\n",
    "        stream1 = self.dropout(stream1)\n",
    "        stream1 = F.relu(self.fc2(stream1))\n",
    "        \n",
    "        stream2 = F.relu(self.fc3(x))\n",
    "        stream2 = self.dropout(stream2)\n",
    "        stream2 = F.relu(self.fc4(stream2))\n",
    "        \n",
    "        combined = torch.cat((stream1, stream2), -1)\n",
    "        outputs = torch.sigmoid(self.fc5(combined))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e38a1c50-91e2-443d-aee2-b6dd36b64d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = spectral_net(pbmc.shape[1], nhidden=32, p=8, output_dim=labels_df.shape[1], adj=V, batch_size=batch_size, dropout=0.3)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b019c1cb-fe84-4218-b900-7df1ff7658c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                             lr = 1e-3)\n",
    "\n",
    "x_tensor = torch.tensor(pbmc[:len(pbmc)-23].astype(np.float32))\n",
    "y_tensor = torch.tensor(labels_df[:len(pbmc)-23].values.astype(np.float32))\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# batch_size = 32\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 301\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(x_tensor)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "bd91e2d9-6e89-43ed-980b-5a71bf67cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = math.floor(len(dataloader.dataset)*0.8)\n",
    "    model.train()\n",
    "    correct = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            correct /= 100*batch_size\n",
    "\n",
    "            print(f\"Train accuracy: {(100*correct):>0.1f}%, loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            correct = 0\n",
    "            \n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = math.floor(len(dataloader.dataset)*0.2)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    print(correct)\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "59b3aae4-4ae0-4f60-bae7-db2c1d9e44b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj tensor(1.6146) tensor(0.0098)\n",
      "gcn.weight tensor(0.5872, grad_fn=<MaxBackward1>) tensor(0.1212, grad_fn=<VarBackward0>)\n",
      "fc1.weight tensor(0.0204, grad_fn=<MaxBackward1>) tensor(8.6922e-05, grad_fn=<VarBackward0>)\n",
      "fc1.bias tensor(0.0161, grad_fn=<MaxBackward1>) tensor(8.6510e-05, grad_fn=<VarBackward0>)\n",
      "fc2.weight tensor(0.0998, grad_fn=<MaxBackward1>) tensor(0.0026, grad_fn=<VarBackward0>)\n",
      "fc2.bias tensor(0.0842, grad_fn=<MaxBackward1>) tensor(0.0021, grad_fn=<VarBackward0>)\n",
      "fc3.weight tensor(0.0323, grad_fn=<MaxBackward1>) tensor(0.0003, grad_fn=<VarBackward0>)\n",
      "fc3.bias tensor(0.0319, grad_fn=<MaxBackward1>) tensor(0.0004, grad_fn=<VarBackward0>)\n",
      "fc4.weight tensor(0.0884, grad_fn=<MaxBackward1>) tensor(0.0026, grad_fn=<VarBackward0>)\n",
      "fc4.bias tensor(0.0839, grad_fn=<MaxBackward1>) tensor(0.0029, grad_fn=<VarBackward0>)\n",
      "fc5.weight tensor(0.1897, grad_fn=<MaxBackward1>) tensor(0.0107, grad_fn=<VarBackward0>)\n",
      "fc5.bias tensor(0.1445, grad_fn=<MaxBackward1>) tensor(0.0127, grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.max(), param.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141364be-8241-4410-88e4-e31bc0bfd60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = adj_mat.shape[0] # number of nodes in a graph\n",
    "D = np.sum(adj_mat, 0) # node degrees\n",
    "D_hat = np.diag((D + 1e-5)**(-0.5)) # normalized node degrees\n",
    "L = np.identity(N) - np.dot(D_hat, adj_mat).dot(D_hat) # Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a10c702-2764-4bdb-b8e3-afb5032eb158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(967, 967)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c709237-90fe-4bd4-91e3-3fa8bb96ba85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(962, 962)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f783d7e3-6ee1-4920-b154-18f22e0b5b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"adj_mat.csv\", adj_mat, delimiter=\",\", fmt='%d') #save one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8162abd-5fb7-43bf-969c-5b84176614aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Overview of Gene-Gene network from STRING database\n",
    "\n",
    "G = nx.from_numpy_matrix(adj_mat)\n",
    "nx.draw_networkx(G, node_size=20, with_labels=False, linewidths=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98213ce8-fa15-4496-a343-96f3e437316f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
